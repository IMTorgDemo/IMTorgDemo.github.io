<!DOCTYPE html>
<html>
<head>
    <title>Neural Network Basics: Linear Regression with PyTorch // IMTorg Kbase</title>

        <meta charset="utf-8">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="author" content="">
        <meta property="og:title" content="Neural Network Basics: Linear Regression with PyTorch" />
    <meta property="og:description" content="" />
    <meta property="og:type" content="website" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:url" content="https://imtorgdemo.github.io/posts/blog_models-neuralnet_pytorch_intro/" />
    

    <link href="" rel="alternate" type="application/rss+xml" title="IMTorg Kbase" />
    
    
    
    <link rel="apple-touch-icon" sizes="57x57" href="/favico/apple-icon-57x57.png">
	<link rel="apple-touch-icon" sizes="60x60" href="/favico/apple-icon-60x60.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/favico/apple-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="76x76" href="/favico/apple-icon-76x76.png">
	<link rel="apple-touch-icon" sizes="114x114" href="/favico/apple-icon-114x114.png">
	<link rel="apple-touch-icon" sizes="120x120" href="/favico/apple-icon-120x120.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/favico/apple-icon-144x144.png">
	<link rel="apple-touch-icon" sizes="152x152" href="/favico/apple-icon-152x152.png">
	<link rel="apple-touch-icon" sizes="180x180" href="/favico/apple-icon-180x180.png">
	<link rel="icon" type="image/png" sizes="192x192"  href="/android-icon-192x192.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favico/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favico/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favico/favicon-16x16.png">
	<link rel="manifest" href="/images/favico/manifest.json">
	<meta name="msapplication-TileColor" content="#ffffff">
	<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
	<meta name="theme-color" content="#ffffff">
    

    <link href="https://imtorgdemo.github.io/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
    <link href="https://imtorgdemo.github.io/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="https://imtorgdemo.github.io/css/style.css">

    <meta name="generator" content="Hugo 0.101.0" />
</head>


<body>
<div id="container">
    <header id="header">
    <div id="header-outer" class="outer">
        <div id="header-inner" class="inner">
            <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
            
            <a href="https://imtorgdemo.github.io/">
            	<img src="/logo_CA_newblue.png" alt="Logo" style="max-width:100px; padding-top: 10px">
            </a>
            <nav id="main-nav">
                
                <a class="main-nav-link" href="/pages/about/">About</a>
                
                <a class="main-nav-link" href="/categories/">Categories</a>
                
                <a class="main-nav-link" href="/pages/search/">Search</a>
                
            </nav>
            <nav id="sub-nav">
                <div id="search-form-wrap">
                </div>
            </nav>
        </div>
    </div>
</header>

    <section id="main" class="outer">
        <article class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        <header class="article-header">
            <h1 class="article-title" itemprop="name">Neural Network Basics: Linear Regression with PyTorch</h1>
        </header>
        
        <div class="article-meta">
            <a href="/posts/blog_models-neuralnet_pytorch_intro/" class="article-date">
                <time datetime='2020-07-01T00:00:00.000&#43;00:00' itemprop="datePublished">2020-07-01</time>
            </a>
            
            
            <div class="post-categories">
                <div class="article-category">
                    
                    
                    <a class="article-category-link" href="https://imtorgdemo.github.io//categories/introduction_tutorial">Introduction_Tutorial</a>
                    
                    
                    <span>&gt;</span>
                    
                    <a class="article-category-link" href="https://imtorgdemo.github.io//categories/data_science">Data_Science</a>
                    
                </div>
            </div>
            
            
        </div>
        <div class="article-entry" itemprop="articleBody">
            <p>In just a few short years, PyTorch took the crown for most popular deep learning framework.  Its concise and straightforward API allows for custom changes to popular networks and layers.  While some of the descriptions may some foreign to mathematicians, the concepts are familiar to anyone with a little experience in machine learning.  This post will walk the user from a simple linear regression to an (overkill) neural network model, with thousands of parameters, which provides a good base for future learning.</p>
<h2 id="configuration">Configuration</h2>
<p>Setup our environment with the basic libraries and necessary data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>set_printoptions(edgeitems<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>c <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.5</span>,  <span style="color:#ae81ff">14.0</span>, <span style="color:#ae81ff">15.0</span>, <span style="color:#ae81ff">28.0</span>, <span style="color:#ae81ff">11.0</span>,  <span style="color:#ae81ff">8.0</span>,  <span style="color:#ae81ff">3.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">4.0</span>,  <span style="color:#ae81ff">6.0</span>, <span style="color:#ae81ff">13.0</span>, <span style="color:#ae81ff">21.0</span>]
</span></span><span style="display:flex;"><span>u <span style="color:#f92672">=</span> [<span style="color:#ae81ff">35.7</span>, <span style="color:#ae81ff">55.9</span>, <span style="color:#ae81ff">58.2</span>, <span style="color:#ae81ff">81.9</span>, <span style="color:#ae81ff">56.3</span>, <span style="color:#ae81ff">48.9</span>, <span style="color:#ae81ff">33.9</span>, <span style="color:#ae81ff">21.8</span>, <span style="color:#ae81ff">48.4</span>, <span style="color:#ae81ff">60.4</span>, <span style="color:#ae81ff">68.4</span>]
</span></span></code></pre></div><h2 id="progressive-estimation">Progressive Estimation</h2>
<h3 id="estimation-qr-decomposition">Estimation: QR Decomposition</h3>
<p>R is the Domain Specific Language for statistics, and we will use R&rsquo;s well-known <code>lm()</code> function for making initial estimates for later comparisons.  The <code>lm()</code> function uses QR decomposition for solving the normal equations for the parameters.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;c&#39;</span>: c, <span style="color:#e6db74">&#39;u&#39;</span>: u })
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">%</span>load_ext rpy2<span style="color:#f92672">.</span>ipython
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#f92672">%%</span>R <span style="color:#f92672">-</span>i df <span style="color:#f92672">-</span>w <span style="color:#ae81ff">400</span> <span style="color:#f92672">-</span>h <span style="color:#ae81ff">300</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>f <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(c <span style="color:#f92672">~</span> u, df)
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">print</span>( <span style="color:#a6e22e">summary</span>(f)<span style="color:#f92672">$</span>coeff )
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">print</span>( <span style="color:#a6e22e">sprintf</span>(<span style="color:#e6db74">&#34;MSE=%0.2f&#34;</span>, <span style="color:#a6e22e">mean</span>(f<span style="color:#f92672">$</span>residuals^2) ))
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">plot</span>(c <span style="color:#f92672">~</span> u, df)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">               Estimate Std. Error  t value     Pr(&gt;|t|)
(Intercept) -17.3047855  1.9272273 -8.97911 8.701879e-06
u             0.5367719  0.0355386 15.10391 1.062435e-07
[1] &#34;MSE=2.93&#34;
</code></pre>

</div>

<p><img src="output_9_1.png" alt="png"></p>
<h3 id="prepare-tensors">Prepare tensors</h3>
<p>Lets create PyTorch tensors out of our data and create basic implementations of the model and loss functions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>t_c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(c)
</span></span><span style="display:flex;"><span>t_u <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(u)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">model</span>(t_u, w, b):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> w <span style="color:#f92672">*</span> t_u <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(t_p, t_c):
</span></span><span style="display:flex;"><span>    squared_diffs <span style="color:#f92672">=</span> (t_p <span style="color:#f92672">-</span> t_c)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> squared_diffs<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#initialize parameters</span>
</span></span><span style="display:flex;"><span>w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(())
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>t_p <span style="color:#f92672">=</span> model(t_u, w, b)
</span></span><span style="display:flex;"><span>t_p
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> loss_fn(t_p, t_c)
</span></span><span style="display:flex;"><span>loss
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor(1763.8846)
</code></pre>

</div>

<h3 id="naive-gd-algorithm">Naive GD algorithm</h3>
<p>The naive gradient descent algorithm displays the basic idea for updating parameter estimates over a solution surface, but this is too simple for a solution.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>delta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>loss_rate_of_change_w <span style="color:#f92672">=</span> \
</span></span><span style="display:flex;"><span>    (loss_fn(model(t_u, w <span style="color:#f92672">+</span> delta, b), t_c) <span style="color:#f92672">-</span> 
</span></span><span style="display:flex;"><span>     loss_fn(model(t_u, w <span style="color:#f92672">-</span> delta, b), t_c)) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2.0</span> <span style="color:#f92672">*</span> delta)
</span></span><span style="display:flex;"><span>w <span style="color:#f92672">=</span> w <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> loss_rate_of_change_w
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>loss_rate_of_change_b <span style="color:#f92672">=</span> \
</span></span><span style="display:flex;"><span>    (loss_fn(model(t_u, w, b <span style="color:#f92672">+</span> delta), t_c) <span style="color:#f92672">-</span> 
</span></span><span style="display:flex;"><span>     loss_fn(model(t_u, w, b <span style="color:#f92672">-</span> delta), t_c)) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2.0</span> <span style="color:#f92672">*</span> delta)
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> b <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> loss_rate_of_change_b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(w, b)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor(-44.1730) tensor(46.0250)
</code></pre>

</div>

<h3 id="analytical-gd-method">Analytical GD method</h3>
<p>We can create a gradient function, analytically, by taking derivates (chain rule) with respect to the parameters.  A longer derivation can be found in &lsquo;The Elements of Statistical Learning&rsquo;, but the gist is that updates can be done in 2 passes:</p>
<ul>
<li>forward: fixed weights are used to compute predicted values</li>
<li>backward:
<ul>
<li>errors (delta) are computed</li>
<li>errors (s) are &lsquo;back-propogated&rsquo; using the back-propogation equation</li>
<li>both delta and s errors are used to compute gradients the updates in gradient descent</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dloss_fn</span>(t_p, t_c):
</span></span><span style="display:flex;"><span>    dsq_diffs <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> (t_p <span style="color:#f92672">-</span> t_c) <span style="color:#f92672">/</span> t_p<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># &lt;1&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dsq_diffs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dmodel_dw</span>(t_u, w, b):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> t_u
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dmodel_db</span>(t_u, w, b):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_fn</span>(t_u, t_c, t_p, w, b):
</span></span><span style="display:flex;"><span>    dloss_dtp <span style="color:#f92672">=</span> dloss_fn(t_p, t_c)
</span></span><span style="display:flex;"><span>    dloss_dw <span style="color:#f92672">=</span> dloss_dtp <span style="color:#f92672">*</span> dmodel_dw(t_u, w, b)
</span></span><span style="display:flex;"><span>    dloss_db <span style="color:#f92672">=</span> dloss_dtp <span style="color:#f92672">*</span> dmodel_db(t_u, w, b)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>stack([dloss_dw<span style="color:#f92672">.</span>sum(), dloss_db<span style="color:#f92672">.</span>sum()])  <span style="color:#75715e"># &lt;1&gt;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_loop</span>(n_epochs, learning_rate, params, t_u, t_c, print_params<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, n_epochs <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        w, b <span style="color:#f92672">=</span> params
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        t_p <span style="color:#f92672">=</span> model(t_u, w, b)                 <span style="color:#75715e">#Forward Pass (prediction)</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fn(t_p, t_c)
</span></span><span style="display:flex;"><span>        grad <span style="color:#f92672">=</span> grad_fn(t_u, t_c, t_p, w, b)    <span style="color:#75715e">#Backward Pass (compute gradient)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        params <span style="color:#f92672">=</span> params <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> grad
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">in</span> {<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">99</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">4000</span>, <span style="color:#ae81ff">5000</span>}:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#39;Epoch </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, Loss </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> (epoch, float(loss)))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> print_params:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">&#39;    Params:&#39;</span>, params)
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">&#39;    Grad:  &#39;</span>, grad)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">in</span> {<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">101</span>}:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#39;...&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> torch<span style="color:#f92672">.</span>isfinite(loss)<span style="color:#f92672">.</span>all():     <span style="color:#75715e">#check for divergence (updates are too large)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span> 
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> params
</span></span></code></pre></div><p>Fix divergence with different approaches, including:</p>
<ul>
<li>ensure similar parameters (normalize input columns)</li>
<li>adaptive learning rate</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>t_un <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#f92672">*</span> t_u
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>params <span style="color:#f92672">=</span> training_loop(
</span></span><span style="display:flex;"><span>    n_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>, 
</span></span><span style="display:flex;"><span>    learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-2</span>, 
</span></span><span style="display:flex;"><span>    params <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>]), 
</span></span><span style="display:flex;"><span>    t_u <span style="color:#f92672">=</span> t_un, 
</span></span><span style="display:flex;"><span>    t_c <span style="color:#f92672">=</span> t_c,
</span></span><span style="display:flex;"><span>    print_params <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>params
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Epoch 1, Loss 80.364342
Epoch 2, Loss 37.574917
Epoch 3, Loss 30.871077
...
Epoch 10, Loss 29.030487
Epoch 11, Loss 28.941875
...
Epoch 99, Loss 22.214186
Epoch 100, Loss 22.148710
...
Epoch 4000, Loss 2.927680
Epoch 5000, Loss 2.927648
</code></pre>

</div>

<div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([  5.3671, -17.3012])
</code></pre>

</div>

<h2 id="pytorch-components">PyTorch Components</h2>
<p>The PyTorch API is well designed, but there are many assumptions incorporated into the functionality.  Be sure you know these basics, thoroughly.</p>
<h3 id="using-autograd">Using autograd</h3>
<p>Back-propagation: we computed the gradient of a composition of functions - the model and the loss - with respect to their inner-most parameters - w and b - by propagating derivatives backwards using the chain rule.</p>
<p>Given a forward expression, no matter how nested, PyTorch will provide the gradient of that expression with respect to its input parameters automatically.  This is because PyTorch tensors can remember where they come from, in terms of the operations and parent tensors that originated them, and they can provide the chain of derivatives of such operations with respect to their inputs automatically.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">model</span>(t_u, w, b):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> w <span style="color:#f92672">*</span> t_u <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(t_p, t_c):
</span></span><span style="display:flex;"><span>    squared_diffs <span style="color:#f92672">=</span> (t_p <span style="color:#f92672">-</span> t_c)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> squared_diffs<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>params <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>params<span style="color:#f92672">.</span>grad <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">True
</code></pre>

</div>

<p>That argument <code>requires_grad=True</code> is telling PyTorch to track the entire family tree of tensors resulting from operations on params. In other words, any tensor that will have params as an ancestor will have access to the chain of functions that were called to get from params to that tensor. In case these functions are differentiable (and most PyTorch tensor operations will be), the value of the derivative will be automatically populated as a grad attribute of the params tensor.</p>
<p>All we have to do to populate it is to start with a tensor with requires_grad set to True, then call the model (predict new values), compute the loss, and then call backward on the loss tensor.  The grad attribute of params contains the derivatives of the loss with respect to each element of params.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> loss_fn(model(t_u, <span style="color:#f92672">*</span>params), t_c)
</span></span><span style="display:flex;"><span>loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>params<span style="color:#f92672">.</span>grad
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([4517.2969,   82.6000])
</code></pre>

</div>

<p><strong>WARNING:</strong> Calling backward will lead derivatives to accumulate (summed) at leaf nodes. We need to zero the gradient explicitly after using it for parameter updates.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> params<span style="color:#f92672">.</span>grad <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    params<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>params<span style="color:#f92672">.</span>grad
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([0., 0.])
</code></pre>

</div>

<p>The logic inside the <code>with</code> statement will be used with an &lsquo;optimizer&rsquo;.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_loop</span>(n_epochs, learning_rate, params, t_u, t_c):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, n_epochs <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> params<span style="color:#f92672">.</span>grad <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:     <span style="color:#75715e">#call prior to loss.backward()</span>
</span></span><span style="display:flex;"><span>            params<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        t_p <span style="color:#f92672">=</span> model(t_u, <span style="color:#f92672">*</span>params) 
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fn(t_p, t_c)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():          <span style="color:#75715e">#autograd mechanism should not add edges to the forward graph </span>
</span></span><span style="display:flex;"><span>            params <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> params<span style="color:#f92672">.</span>grad        <span style="color:#75715e">#keep the same tensor params around, but subtract our update from it</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">%</span> <span style="color:#ae81ff">500</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#39;Epoch </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, Loss </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> (epoch, float(loss)))
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> params
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>training_loop(
</span></span><span style="display:flex;"><span>    n_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>,
</span></span><span style="display:flex;"><span>    learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-2</span>,
</span></span><span style="display:flex;"><span>    params <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>    t_u <span style="color:#f92672">=</span> t_un,
</span></span><span style="display:flex;"><span>    t_c <span style="color:#f92672">=</span> t_c)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Epoch 500, Loss 7.860116
Epoch 1000, Loss 3.828538
Epoch 1500, Loss 3.092191
Epoch 2000, Loss 2.957697
Epoch 2500, Loss 2.933134
Epoch 3000, Loss 2.928648
Epoch 3500, Loss 2.927830
Epoch 4000, Loss 2.927679
Epoch 4500, Loss 2.927652
Epoch 5000, Loss 2.927647
</code></pre>

</div>

<div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([  5.3671, -17.3012], requires_grad=True)
</code></pre>

</div>

<h3 id="selecting-an-optimizer">Selecting an optimizer</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dir(optim)[:<span style="color:#ae81ff">10</span>]
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">[&#39;ASGD&#39;,
 &#39;Adadelta&#39;,
 &#39;Adagrad&#39;,
 &#39;Adam&#39;,
 &#39;AdamW&#39;,
 &#39;Adamax&#39;,
 &#39;LBFGS&#39;,
 &#39;Optimizer&#39;,
 &#39;RMSprop&#39;,
 &#39;Rprop&#39;]
</code></pre>

</div>

<p>The optimizer is used with four basic steps:</p>
<ul>
<li>optimizer holds a reference to parameters, and</li>
<li>after a loss is computed from inputs</li>
<li>a call to <code>.backward()</code> leads to <code>.grad()</code> being populated on parameters, then</li>
<li>the optimizer can access <code>.grad()</code> and compute the parameter updates</li>
</ul>
<p>The optimizer exposes two methods:</p>
<ul>
<li><code>.zero_grad()</code> - zeroes the grad attribute of all the parameters passed to the optimizer</li>
<li><code>.step()</code> - updates the value of those parameters according to the specific optimization strategy</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>params <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-5</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD([params], lr<span style="color:#f92672">=</span>learning_rate)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#complete step</span>
</span></span><span style="display:flex;"><span>t_p <span style="color:#f92672">=</span> model(t_un, <span style="color:#f92672">*</span>params)
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> loss_fn(t_p, t_c)
</span></span><span style="display:flex;"><span>optimizer<span style="color:#f92672">.</span>zero_grad()    <span style="color:#75715e">#must zero-out params</span>
</span></span><span style="display:flex;"><span>loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>params
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([1.0008e+00, 1.0640e-04], requires_grad=True)
</code></pre>

</div>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_loop</span>(n_epochs, optimizer, params, t_u, t_c):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, n_epochs <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        t_p <span style="color:#f92672">=</span> model(t_u, <span style="color:#f92672">*</span>params)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fn(t_p, t_c)
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">%</span> <span style="color:#ae81ff">500</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#39;Epoch </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, Loss </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> (epoch, float(loss)))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> params
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>params <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-2</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD([params], lr<span style="color:#f92672">=</span>learning_rate)    <span style="color:#75715e">#same params</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>training_loop(
</span></span><span style="display:flex;"><span>    n_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>,
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optimizer,
</span></span><span style="display:flex;"><span>    params <span style="color:#f92672">=</span> params,        <span style="color:#75715e">#same params</span>
</span></span><span style="display:flex;"><span>    t_u <span style="color:#f92672">=</span> t_un,
</span></span><span style="display:flex;"><span>    t_c <span style="color:#f92672">=</span> t_c)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Epoch 500, Loss 7.860118
Epoch 1000, Loss 3.828538
Epoch 1500, Loss 3.092191
Epoch 2000, Loss 2.957697
Epoch 2500, Loss 2.933134
Epoch 3000, Loss 2.928648
Epoch 3500, Loss 2.927830
Epoch 4000, Loss 2.927680
Epoch 4500, Loss 2.927651
Epoch 5000, Loss 2.927648
</code></pre>

</div>

<div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([  5.3671, -17.3012], requires_grad=True)
</code></pre>

</div>

<h3 id="choosing-activation-functions">Choosing activation functions</h3>
<p>A neural network is actually just a polynomial function with &lsquo;activation&rsquo; functions around the nested terms.  This small list of activation functions gives an idea of the most useful properties.  While sigmoid was the most orthodox, originally, Rectified Linear Units (ReLU) are shown to be better.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>input_t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3.1</span>, <span style="color:#ae81ff">0.1</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>activation_list <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Tanh(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Hardtanh(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Sigmoid(),
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Softplus(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>LeakyReLU(negative_slope<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>),
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#nn.Tanhshrink(),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#nn.Softshrink(),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#nn.Hardshrink(),</span>
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">14</span>, <span style="color:#ae81ff">28</span>), dpi<span style="color:#f92672">=</span><span style="color:#ae81ff">600</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, activation_func <span style="color:#f92672">in</span> enumerate(activation_list):
</span></span><span style="display:flex;"><span>    subplot <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(len(activation_list), <span style="color:#ae81ff">3</span>, i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    subplot<span style="color:#f92672">.</span>set_title(type(activation_func)<span style="color:#f92672">.</span>__name__)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    output_t <span style="color:#f92672">=</span> activation_func(input_t)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>grid()
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(input_t<span style="color:#f92672">.</span>numpy(), input_t<span style="color:#f92672">.</span>numpy(),<span style="color:#e6db74">&#39;k&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot([<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>], <span style="color:#e6db74">&#39;k&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>], [<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>], <span style="color:#e6db74">&#39;k&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(input_t<span style="color:#f92672">.</span>numpy(), output_t<span style="color:#f92672">.</span>numpy(), <span style="color:#e6db74">&#39;r&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>output_t
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([-0.3000, -0.2900, -0.2800, -0.2700, -0.2600, -0.2500, -0.2400, -0.2300,
        -0.2200, -0.2100, -0.2000, -0.1900, -0.1800, -0.1700, -0.1600, -0.1500,
        -0.1400, -0.1300, -0.1200, -0.1100, -0.1000, -0.0900, -0.0800, -0.0700,
        -0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.1000,
         0.2000,  0.3000,  0.4000,  0.5000,  0.6000,  0.7000,  0.8000,  0.9000,
         1.0000,  1.1000,  1.2000,  1.3000,  1.4000,  1.5000,  1.6000,  1.7000,
         1.8000,  1.9000,  2.0000,  2.1000,  2.2000,  2.3000,  2.4000,  2.5000,
         2.6000,  2.7000,  2.8000,  2.9000,  3.0000])
</code></pre>

</div>

<p><img src="output_42_1.png" alt="png"></p>
<h3 id="preparing-the-training-validation-split">Preparing the training-validation split</h3>
<p>The first line in the training loop evaluates model on <code>train_t_u</code> to produce <code>train_t_p</code>. Then <code>train_loss</code> is evaluated from <code>train_t_p</code>. This creates a computation graph that links <code>train_t_u</code> to <code>train_t_p</code> to <code>train_loss</code>. When model is evaluated again on <code>val_t_u</code>, it produces <code>val_t_p</code> and <code>val_loss</code>. In this case, a separate
computation graph will be created that links <code>val_t_u</code> to <code>val_t_p</code> to <code>val_loss</code>. Separate tensors have been run through the same functions, <code>model()</code> and <code>loss_fn()</code>, generating separate computation graphs.</p>
<p>Since we’re never calling <code>backward()</code> on <code>val_loss</code>, why are we building the graph in the first place? We could in fact just call <code>model()</code> and <code>loss_fn()</code> as plain functions, without tracking history. However optimized, tracking history comes with additional costs that we could totally forego during the validation pass, especially when the model has millions of parameters.  In order to address this, PyTorch allows us to <strong>switch off autograd</strong> when we don’t need it using the <code>torch.no_grad</code> context manager.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>n_samples <span style="color:#f92672">=</span> t_u<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>n_val <span style="color:#f92672">=</span> int(<span style="color:#ae81ff">0.2</span> <span style="color:#f92672">*</span> n_samples)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>shuffled_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randperm(n_samples)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_indices <span style="color:#f92672">=</span> shuffled_indices[:<span style="color:#f92672">-</span>n_val]
</span></span><span style="display:flex;"><span>val_indices <span style="color:#f92672">=</span> shuffled_indices[<span style="color:#f92672">-</span>n_val:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_indices, val_indices
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">(tensor([ 8,  5, 10,  0,  1,  6,  3,  2,  7]), tensor([4, 9]))
</code></pre>

</div>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train_t_u <span style="color:#f92672">=</span> t_u[train_indices]
</span></span><span style="display:flex;"><span>train_t_c <span style="color:#f92672">=</span> t_c[train_indices]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>val_t_u <span style="color:#f92672">=</span> t_u[val_indices]
</span></span><span style="display:flex;"><span>val_t_c <span style="color:#f92672">=</span> t_c[val_indices]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_t_un <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#f92672">*</span> train_t_u
</span></span><span style="display:flex;"><span>val_t_un <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#f92672">*</span> val_t_u
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_loop</span>(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, n_epochs <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        train_t_p <span style="color:#f92672">=</span> model(train_t_u, <span style="color:#f92672">*</span>params)       <span style="color:#75715e">#training data</span>
</span></span><span style="display:flex;"><span>        train_loss <span style="color:#f92672">=</span> loss_fn(train_t_p, train_t_c)
</span></span><span style="display:flex;"><span>                             
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():                       <span style="color:#75715e">#switch-off autograd when we don’t need it                         </span>
</span></span><span style="display:flex;"><span>            val_t_p <span style="color:#f92672">=</span> model(val_t_u, <span style="color:#f92672">*</span>params)       <span style="color:#75715e">#validation data, separate computation graph will be created</span>
</span></span><span style="display:flex;"><span>            val_loss <span style="color:#f92672">=</span> loss_fn(val_t_p, val_t_c)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">assert</span> val_loss<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">==</span> <span style="color:#66d9ef">False</span>  <span style="color:#75715e">#ensure autograd is off</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        train_loss<span style="color:#f92672">.</span>backward()          <span style="color:#75715e">#backward only called on train_loss: accumulated the derivatives on the leaf nodes</span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">3</span> <span style="color:#f92672">or</span> epoch <span style="color:#f92672">%</span> <span style="color:#ae81ff">500</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#39;Epoch </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">, Training loss </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">, Validation loss </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(
</span></span><span style="display:flex;"><span>                epoch, float(train_loss), float(val_loss)))
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> params
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>params <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-2</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD([params], lr<span style="color:#f92672">=</span>learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>training_loop(
</span></span><span style="display:flex;"><span>    n_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">3000</span>, 
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optimizer,
</span></span><span style="display:flex;"><span>    params <span style="color:#f92672">=</span> params,
</span></span><span style="display:flex;"><span>    train_t_u <span style="color:#f92672">=</span> train_t_un,        <span style="color:#75715e">#training dependent var</span>
</span></span><span style="display:flex;"><span>    val_t_u <span style="color:#f92672">=</span> val_t_un,            <span style="color:#75715e">#validation dependent var </span>
</span></span><span style="display:flex;"><span>    train_t_c <span style="color:#f92672">=</span> train_t_c,
</span></span><span style="display:flex;"><span>    val_t_c <span style="color:#f92672">=</span> val_t_c)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Epoch 1, Training loss 93.96257781982422, Validation loss 19.172250747680664
Epoch 2, Training loss 30.066646575927734, Validation loss 43.27933120727539
Epoch 3, Training loss 24.354103088378906, Validation loss 53.700531005859375
Epoch 500, Training loss 11.108868598937988, Validation loss 12.03146743774414
Epoch 1000, Training loss 5.690493106842041, Validation loss 0.8163924217224121
Epoch 1500, Training loss 3.397932529449463, Validation loss 1.738821268081665
Epoch 2000, Training loss 2.4279325008392334, Validation loss 5.815708160400391
Epoch 2500, Training loss 2.017514944076538, Validation loss 9.938664436340332
Epoch 3000, Training loss 1.843862771987915, Validation loss 13.242980003356934
</code></pre>

</div>

<div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([  5.8612, -20.4629], requires_grad=True)
</code></pre>

</div>

<p>Our main goal is to also see both the training loss and the validation loss decreasing. While ideally both losses would be rougly the same value, as long as validation loss stays reasonably close to the training loss, we know that our model is continuing to learn generalized things about our data.</p>
<h2 id="working-with-layers-nnmodule">Working with Layers (nn.Module)</h2>
<p>A PyTorch module is a Python class deriving from the <code>nn.Module</code> base class. A Module can have one or more <code>Parameter</code> instances as attributes, which are tensors whose values are optimized during the training process (think w and b in our linear model). A Module can also have one or more submodules (subclasses of <code>nn.Module</code>) as attributes, and it will be able to track their Parameters as well.</p>
<p>NOTE
The submodules must be top-level attributes, not buried inside list or dict instances! Otherwise the optimizer will not be able to locate the submodules (and hence their parameters). For situations where your model requires a list or dict of submodules, PyTorch provides <code>nn.ModuleList</code> and <code>nn.ModuleDict</code>.</p>
<p>All PyTorch-provided subclasses of <code>nn.Module</code> have their <code>__call__</code> method defined. This allows one to instantiate an <code>nn.Linear</code> and call it as if it was a function, like so:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>c <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.5</span>,  <span style="color:#ae81ff">14.0</span>, <span style="color:#ae81ff">15.0</span>, <span style="color:#ae81ff">28.0</span>, <span style="color:#ae81ff">11.0</span>,  <span style="color:#ae81ff">8.0</span>,  <span style="color:#ae81ff">3.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">4.0</span>,  <span style="color:#ae81ff">6.0</span>, <span style="color:#ae81ff">13.0</span>, <span style="color:#ae81ff">21.0</span>]
</span></span><span style="display:flex;"><span>u <span style="color:#f92672">=</span> [<span style="color:#ae81ff">35.7</span>, <span style="color:#ae81ff">55.9</span>, <span style="color:#ae81ff">58.2</span>, <span style="color:#ae81ff">81.9</span>, <span style="color:#ae81ff">56.3</span>, <span style="color:#ae81ff">48.9</span>, <span style="color:#ae81ff">33.9</span>, <span style="color:#ae81ff">21.8</span>, <span style="color:#ae81ff">48.4</span>, <span style="color:#ae81ff">60.4</span>, <span style="color:#ae81ff">68.4</span>]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>t_c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(c)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)     <span style="color:#75715e">#add an extra dimension for batch</span>
</span></span><span style="display:flex;"><span>t_u <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(u)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)     <span style="color:#75715e">#same</span>
</span></span><span style="display:flex;"><span>t_c[:<span style="color:#ae81ff">3</span>]
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([[ 0.5000],
        [14.0000],
        [15.0000]])
</code></pre>

</div>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>linear_model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)     <span style="color:#75715e">#args: input size, output size, and bias defaulting to True.</span>
</span></span><span style="display:flex;"><span>linear_model(t_c[:<span style="color:#ae81ff">3</span>])
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">tensor([[-0.8724],
        [-4.6115],
        [-4.8885]], grad_fn=&lt;AddmmBackward&gt;)
</code></pre>

</div>

<p>Calling an instance of <code>nn.Module</code> with a set of arguments ends up calling a method named forward with the same arguments. The forward method is what executes the forward computation, while <code>__call__</code> does other rather important chores before and after calling forward. So, it is technically possible to call forward directly and it will produce the same output as <code>__call__</code>, but it should not be done from user code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>y <span style="color:#f92672">=</span> model(x)              <span style="color:#75715e">#correct</span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>forward(x)      <span style="color:#75715e">#don&#39;t do this</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>linear_model<span style="color:#f92672">.</span>weight, linear_model<span style="color:#f92672">.</span>bias
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">(Parameter containing:
 tensor([[-0.2770]], requires_grad=True), Parameter containing:
 tensor([-0.7339], requires_grad=True))
</code></pre>

</div>

<p>Any module in <code>nn</code> is written to produce outputs for a batch of multiple inputs at the same time.  Modules expect the zeroth dimension of the input to be the number of samples in the batch.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_loop</span>(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, n_epochs <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        t_p_train <span style="color:#f92672">=</span> model(t_u_train)        <span style="color:#75715e">#add model as an input</span>
</span></span><span style="display:flex;"><span>        loss_train <span style="color:#f92672">=</span> loss_fn(t_p_train, t_c_train)
</span></span><span style="display:flex;"><span>        t_p_val <span style="color:#f92672">=</span> model(t_u_val)
</span></span><span style="display:flex;"><span>        loss_val <span style="color:#f92672">=</span> loss_fn(t_p_val, t_c_val)
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        loss_train<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">or</span> epoch <span style="color:#f92672">%</span> <span style="color:#ae81ff">1000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#39;Epoch </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">, Training loss </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">, Validation loss </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(
</span></span><span style="display:flex;"><span>                epoch, float(loss_train), float(loss_val)))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>linear_model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(
</span></span><span style="display:flex;"><span>    linear_model<span style="color:#f92672">.</span>parameters(),    <span style="color:#75715e">#replaced [params] with this method call </span>
</span></span><span style="display:flex;"><span>    lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-2</span>)
</span></span><span style="display:flex;"><span>training_loop(
</span></span><span style="display:flex;"><span>    n_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">3000</span>,
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optimizer,
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> linear_model,
</span></span><span style="display:flex;"><span>    loss_fn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss(),
</span></span><span style="display:flex;"><span>    t_u_train <span style="color:#f92672">=</span> train_t_un,
</span></span><span style="display:flex;"><span>    t_u_val <span style="color:#f92672">=</span> val_t_un,
</span></span><span style="display:flex;"><span>    t_c_train <span style="color:#f92672">=</span> train_t_c,
</span></span><span style="display:flex;"><span>    t_c_val <span style="color:#f92672">=</span> val_t_c)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print()
</span></span><span style="display:flex;"><span>print(linear_model<span style="color:#f92672">.</span>weight)
</span></span><span style="display:flex;"><span>print(linear_model<span style="color:#f92672">.</span>bias)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Epoch 1, Training loss 184.62193298339844, Validation loss 131.338623046875
Epoch 1000, Training loss 2.996669054031372, Validation loss 6.098757266998291
Epoch 2000, Training loss 2.4377217292785645, Validation loss 6.508167743682861
Epoch 3000, Training loss 2.4287760257720947, Validation loss 6.561643123626709

Parameter containing:
tensor([[5.4813]], requires_grad=True)
Parameter containing:
tensor([-17.4250], requires_grad=True)
</code></pre>

</div>

<h2 id="create-a-neuralnetwork">Create a NeuralNetwork</h2>
<h3 id="simple-models">Simple models</h3>
<p>Let’s build the simplest possible neural network: a linear module, followed by an activation function, feeding into another linear module. The first linear + activation layer is commonly referred to as a <strong>hidden layer</strong> for historical reasons, since its outputs are not observed directly but fed into the output layer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>seq_model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">13</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Tanh(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">13</span>, <span style="color:#ae81ff">1</span>))
</span></span></code></pre></div><p>After calling model.backward() all parameters will be populated with their grad and the optimizer will then update their value accordingly during the optimizer.step() call.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>[print(param<span style="color:#f92672">.</span>shape) <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> seq_model<span style="color:#f92672">.</span>parameters()]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> seq_model<span style="color:#f92672">.</span>named_parameters():
</span></span><span style="display:flex;"><span>    print(name, param<span style="color:#f92672">.</span>shape)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">torch.Size([13, 1])
torch.Size([13])
torch.Size([1, 13])
torch.Size([1])

0.weight torch.Size([13, 1])
0.bias torch.Size([13])
2.weight torch.Size([1, 13])
2.bias torch.Size([1])
</code></pre>

</div>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> OrderedDict
</span></span><span style="display:flex;"><span>namedseq_model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(OrderedDict([
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;hidden_linear&#39;</span>, nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">13</span>)),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;hidden_activation&#39;</span>, nn<span style="color:#f92672">.</span>Tanh()),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;output_linear&#39;</span>, nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">13</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>]))
</span></span><span style="display:flex;"><span>namedseq_model<span style="color:#f92672">.</span>output_linear<span style="color:#f92672">.</span>bias
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Parameter containing:
tensor([-0.1054], requires_grad=True)
</code></pre>

</div>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(
</span></span><span style="display:flex;"><span>    seq_model<span style="color:#f92672">.</span>parameters(), 
</span></span><span style="display:flex;"><span>    lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>training_loop(
</span></span><span style="display:flex;"><span>    n_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>,
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optimizer,
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> seq_model,
</span></span><span style="display:flex;"><span>    loss_fn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss(),
</span></span><span style="display:flex;"><span>    t_u_train <span style="color:#f92672">=</span> train_t_un,
</span></span><span style="display:flex;"><span>    t_u_val <span style="color:#f92672">=</span> val_t_un,
</span></span><span style="display:flex;"><span>    t_c_train <span style="color:#f92672">=</span> train_t_c,
</span></span><span style="display:flex;"><span>    t_c_val <span style="color:#f92672">=</span> val_t_c)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;output&#39;</span>, seq_model(val_t_un))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;answer&#39;</span>, val_t_c)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;hidden&#39;</span>, seq_model<span style="color:#f92672">.</span>hidden_linear<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>grad)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Epoch 1, Training loss 1.2322325706481934, Validation loss 7.267159461975098
Epoch 1000, Training loss 1.2229273319244385, Validation loss 7.296848773956299
Epoch 2000, Training loss 1.2134442329406738, Validation loss 7.330402374267578
Epoch 3000, Training loss 1.2036887407302856, Validation loss 7.36702823638916
Epoch 4000, Training loss 1.193618655204773, Validation loss 7.405879974365234
Epoch 5000, Training loss 1.1832159757614136, Validation loss 7.446534156799316

output tensor([[13.1834],
        [16.1821]], grad_fn=&lt;AddmmBackward&gt;)
answer tensor([[11.],
        [13.]])
hidden tensor([[ 0.0005],
        [-0.0007],
        [ 0.0138],
        [ 0.0057],
        [-0.0029],
        [ 0.0027],
        [ 0.0025],
        [-0.0165],
        [ 0.0027],
        [-0.0030],
        [-0.0018],
        [ 0.0017],
        [-0.0152]])
</code></pre>

</div>

<h3 id="subclassing-the-nnmodule">Subclassing the <code>nn.Module</code></h3>
<p>In order to subclass nn.Module, at a minimum we need to define a .forward(&hellip;) function that takes the input to the module and returns the output. If we use standard torch operations, autograd will take care of the backward pass automatically.  Often your entire model will be implemented as a subclass of nn.Module, which can, in turn, contain submodules that are also subclasses of nn.Module.</p>
<p>Assigning an instance of nn.Module to an attribute in a nn.Module, just like we did in the constructor here, automatically registers the module as a submodule. This allows modules to have access to the parameters of its submodules without further action by the user.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SubclassModel</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()        <span style="color:#75715e">#calls nn.Module &#39;s __init__ which sets up the housekeeping</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">13</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_activation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Tanh()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">13</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input):
</span></span><span style="display:flex;"><span>        hidden_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>hidden_linear(input)
</span></span><span style="display:flex;"><span>        activated_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>hidden_activation(hidden_t)
</span></span><span style="display:flex;"><span>        output_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>output_linear(activated_t)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output_t
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>subclass_model <span style="color:#f92672">=</span> SubclassModel()
</span></span><span style="display:flex;"><span>subclass_model
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">SubclassModel(
  (hidden_linear): Linear(in_features=1, out_features=13, bias=True)
  (hidden_activation): Tanh()
  (output_linear): Linear(in_features=13, out_features=1, bias=True)
)
</code></pre>

</div>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> type_str, model <span style="color:#f92672">in</span> [(<span style="color:#e6db74">&#39;seq&#39;</span>, seq_model), (<span style="color:#e6db74">&#39;namedseq&#39;</span>, namedseq_model), (<span style="color:#e6db74">&#39;subclass&#39;</span>, subclass_model)]:
</span></span><span style="display:flex;"><span>    print(type_str)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name_str, param <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_parameters():
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{:21}</span><span style="color:#e6db74"> </span><span style="color:#e6db74">{:19}</span><span style="color:#e6db74"> </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(name_str, str(param<span style="color:#f92672">.</span>shape), param<span style="color:#f92672">.</span>numel()))
</span></span><span style="display:flex;"><span>    print()
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">seq
hidden_linear.weight  torch.Size([13, 1]) 13
hidden_linear.bias    torch.Size([13])    13
output_linear.weight  torch.Size([1, 13]) 13
output_linear.bias    torch.Size([1])     1

namedseq
hidden_linear.weight  torch.Size([13, 1]) 13
hidden_linear.bias    torch.Size([13])    13
output_linear.weight  torch.Size([1, 13]) 13
output_linear.bias    torch.Size([1])     1

subclass
hidden_linear.weight  torch.Size([13, 1]) 13
hidden_linear.bias    torch.Size([13])    13
output_linear.weight  torch.Size([1, 13]) 13
output_linear.bias    torch.Size([1])     1
</code></pre>

</div>

<h3 id="using-the-functional-api">Using the functional API</h3>
<p><code>torch.nn.functional</code> provides the many of the same modules we find in nn, but with all eventual parameters moved as an argument to the function call.  By &ldquo;functional&rdquo; here we mean &ldquo;having no internal state&rdquo;, or, in other words, &ldquo;whose output value is solely and fully determined by the value input arguments&rdquo;.  The functional counterpart of <code>nn.Linear</code> is <code>nn.functional.linear</code>.</p>
<p>A Module is a container for state in forms of Parameters and submodules combined with the instructions to do a forward.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SubclassFunctionalModel</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">14</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">14</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input):
</span></span><span style="display:flex;"><span>        hidden_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>hidden_linear(input)
</span></span><span style="display:flex;"><span>        activated_t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tanh(hidden_t)       <span style="color:#75715e">#use functional form, no state is needed</span>
</span></span><span style="display:flex;"><span>        output_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>output_linear(activated_t)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output_t
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>func_model <span style="color:#f92672">=</span> SubclassFunctionalModel()
</span></span><span style="display:flex;"><span>func_model
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">SubclassFunctionalModel(
  (hidden_linear): Linear(in_features=1, out_features=14, bias=True)
  (output_linear): Linear(in_features=14, out_features=1, bias=True)
)
</code></pre>

</div>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(
</span></span><span style="display:flex;"><span>    func_model<span style="color:#f92672">.</span>parameters(), 
</span></span><span style="display:flex;"><span>    lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>training_loop(
</span></span><span style="display:flex;"><span>    n_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>,
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optimizer,
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> func_model,
</span></span><span style="display:flex;"><span>    loss_fn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss(),
</span></span><span style="display:flex;"><span>    t_u_train <span style="color:#f92672">=</span> train_t_un,
</span></span><span style="display:flex;"><span>    t_u_val <span style="color:#f92672">=</span> val_t_un,
</span></span><span style="display:flex;"><span>    t_c_train <span style="color:#f92672">=</span> train_t_c,
</span></span><span style="display:flex;"><span>    t_c_val <span style="color:#f92672">=</span> val_t_c)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;output&#39;</span>, seq_model(val_t_un))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;answer&#39;</span>, val_t_c)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;hidden&#39;</span>, seq_model<span style="color:#f92672">.</span>hidden_linear<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>grad)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Epoch 1, Training loss 179.48944091796875, Validation loss 125.14144897460938
Epoch 1000, Training loss 2.4361188411712646, Validation loss 9.148082733154297
Epoch 2000, Training loss 1.422125220298767, Validation loss 7.820898532867432
Epoch 3000, Training loss 1.3350441455841064, Validation loss 7.297579765319824
Epoch 4000, Training loss 1.3103053569793701, Validation loss 7.1512041091918945
Epoch 5000, Training loss 1.2794909477233887, Validation loss 7.114054203033447

output tensor([[13.1834],
        [16.1821]], grad_fn=&lt;AddmmBackward&gt;)
answer tensor([[11.],
        [13.]])
hidden tensor([[ 0.0005],
        [-0.0007],
        [ 0.0138],
        [ 0.0057],
        [-0.0029],
        [ 0.0027],
        [ 0.0025],
        [-0.0165],
        [ 0.0027],
        [-0.0030],
        [-0.0018],
        [ 0.0017],
        [-0.0152]])
</code></pre>

</div>

<h2 id="conclusion">Conclusion</h2>
<p>This post describes the fundamentals of PyTorch neural networks as they are applied to a simple linear regression.  Because of implicit aspects of this functionality, these must be understood before trying more challenging problems.</p>

        </div>

        
        
        <div class="article-toc" style="display:none;">
            <h3>Contents</h3>
            <nav id="TableOfContents">
  <ul>
    <li><a href="#configuration">Configuration</a></li>
    <li><a href="#progressive-estimation">Progressive Estimation</a>
      <ul>
        <li><a href="#estimation-qr-decomposition">Estimation: QR Decomposition</a></li>
        <li><a href="#prepare-tensors">Prepare tensors</a></li>
        <li><a href="#naive-gd-algorithm">Naive GD algorithm</a></li>
        <li><a href="#analytical-gd-method">Analytical GD method</a></li>
      </ul>
    </li>
    <li><a href="#pytorch-components">PyTorch Components</a>
      <ul>
        <li><a href="#using-autograd">Using autograd</a></li>
        <li><a href="#selecting-an-optimizer">Selecting an optimizer</a></li>
        <li><a href="#choosing-activation-functions">Choosing activation functions</a></li>
        <li><a href="#preparing-the-training-validation-split">Preparing the training-validation split</a></li>
      </ul>
    </li>
    <li><a href="#working-with-layers-nnmodule">Working with Layers (nn.Module)</a></li>
    <li><a href="#create-a-neuralnetwork">Create a NeuralNetwork</a>
      <ul>
        <li><a href="#simple-models">Simple models</a></li>
        <li><a href="#subclassing-the-nnmodule">Subclassing the <code>nn.Module</code></a></li>
        <li><a href="#using-the-functional-api">Using the functional API</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
        </div>
        
        

        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.slim.min.js" integrity="sha256-/SIrNqv8h6QGKDuNoLGA4iret+kyesCkHGzVUUV0shc=" crossorigin="anonymous"></script>
        <script>
            (function() {
                var $toc = $('#TableOfContents');
                if ($toc.length > 0) {
                    var $window = $(window);

                    function onScroll(){
                        var currentScroll = $window.scrollTop();
                        var h = $('.article-entry h1, .article-entry h2, .article-entry h3, .article-entry h4, .article-entry h5, .article-entry h6');
                        var id = "";
                        h.each(function (i, e) {
                            e = $(e);
                            if (e.offset().top - 10 <= currentScroll) {
                                id = e.attr('id');
                            }
                        });
                        var active = $toc.find('a.active');
                        if (active.length == 1 && active.eq(0).attr('href') == '#' + id) return true;

                        active.each(function (i, e) {
                            $(e).removeClass('active').siblings('ul').hide();
                        });
                        $toc.find('a[href="#' + id + '"]').parentsUntil('#TableOfContents').each(function (i, e) {
                            $(e).children('a').addClass('active').siblings('ul').show();
                        });
                    }

                    $window.on('scroll', onScroll);
                    $(document).ready(function() {
                        $toc.find('a').parent('li').find('ul').hide();
                        onScroll();
                        document.getElementsByClassName('article-toc')[0].style.display = '';
                    });
                }
            })();
        </script>
        


        
        <footer class="article-footer">
            <ul class="article-tag-list">
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="https://imtorgdemo.github.io//tags/pytorch">pytorch
                    </a>
                </li>
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="https://imtorgdemo.github.io//tags/python">python
                    </a>
                </li>
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="https://imtorgdemo.github.io//tags/regression">regression
                    </a>
                </li>
                
            </ul>
        </footer>
        
    </div>
    <nav id="article-nav">
    
    <a href="/posts/blog_models-pytorch_computervision/" id="article-nav-newer" class="article-nav-link-wrap">
        <div class="article-nav-title"><span>&lt;</span>&nbsp;
            Computer Vision Using PyTorch
        </div>
    </a>
    
    
    <a href="/posts/blog_bigdata_pyspark-refresher/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">PySpark Refresher Tutorial&nbsp;<span>&gt;</span></div>
    </a>
    
</nav>

</article>

        
    </section>
    <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2022 IMTorg Kbase
            <br />
            <b> Want to discuss software?</b><br>Send me a message <a href="mailto:information@mgmt-tech.org?Subject=Open%20Software" target="_top"></a> information@mgmt-tech.org
        </div>
    </div>
    

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/tomorrow-night.min.css" integrity="sha256-2wL88NKUqvJi/ExflDzkzUumjUM73mcK2gBvBBeLvTk=" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js" integrity="sha256-KbfTjB0WZ8vvXngdpJGY3Yp3xKk+tttbqClO11anCIU=" crossorigin="anonymous"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script>
        <script>renderMathInElement(document.body);</script>
    
    <script>
        document.getElementById('main-nav-toggle').addEventListener('click', function () {
            var header = document.getElementById('header');
            if (header.classList.contains('mobile-on')) {
                header.classList.remove('mobile-on');
            } else {
                header.classList.add('mobile-on');
            }
        });
    </script>
    <script 
	src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.8/lunr.min.js" 
	integrity="sha256-34Si1Y6llMBKM3G0jQILVeoQKEwuxjbk4zGWXXMT4ps=" 
	crossorigin="anonymous"></script>
	<script src="https://imtorgdemo.github.io/js/search.js"></script>
</footer>
</div>
</body>
</html>
