<!DOCTYPE html>
<html>
<head>
    <title>Generalizing the Machine Learning Process // IMTorg Blog Site</title>

        <meta charset="utf-8">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="author" content="">
        <meta property="og:title" content="Generalizing the Machine Learning Process" />
    <meta property="og:description" content="" />
    <meta property="og:type" content="website" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:url" content="https://imtorgdemo.github.io/posts/post_models-machine_learning/" />
    

    <link href="" rel="alternate" type="application/rss+xml" title="IMTorg Blog Site" />
    <link rel="shortcut icon" href="/favicon.png">

    <link href="https://imtorgdemo.github.io/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
    <link href="https://imtorgdemo.github.io/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="https://imtorgdemo.github.io/css/style.css">

    <meta name="generator" content="Hugo 0.55.5" />
</head>


<body>
<div id="container">
    <header id="header">
    <div id="header-outer" class="outer">
        <div id="header-inner" class="inner">
            <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
            <a id="logo" class="logo-text" href="https://imtorgdemo.github.io/">IMTorg Blog Site</a>
            <nav id="main-nav">
                
                <a class="main-nav-link" href="/about/">About</a>
                
                <a class="main-nav-link" href="/categories/">Categories</a>
                
                <a class="main-nav-link" href="/tags/">Tags</a>
                
            </nav>
            <nav id="sub-nav">
                <div id="search-form-wrap">
                </div>
            </nav>
        </div>
    </div>
</header>

    <section id="main" class="outer">
        <article class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        <header class="article-header">
            <h1 class="article-title" itemprop="name">Generalizing the Machine Learning Process</h1>
        </header>
        
        <div class="article-meta">
            <a href="/posts/post_models-machine_learning/" class="article-date">
                <time datetime='2019-05-09T00:00:00.000&#43;00:00' itemprop="datePublished">2019-05-09</time>
            </a>
            
            
            <div class="post-categories">
                <div class="article-category">
                    
                    
                    <a class="article-category-link" href="https://imtorgdemo.github.io//categories/process">Process</a>
                    
                    
                    <span>&gt;</span>
                    
                    <a class="article-category-link" href="https://imtorgdemo.github.io//categories/datascience">DataScience</a>
                    
                </div>
            </div>
            
            
        </div>
        <div class="article-entry" itemprop="articleBody">
            

<p>This work describes a general approach to follow when performing machine learning (ML) manually, and when automating in a deployment setting.  Unlike a classical statistical analysis, standard machine learning projects typically follow a general and repeatable process.  While the practicioner should be aware of details for each of the steps and the reasons for choosing them, there is much less design-thinking and checking of assumptions that are necessary components of more mathematical modeling fields.  This makes the machine learning process amenable to deployment as a service because automating the re-training and prediction of a model with consistent data is straight-forward programming.</p>

<h2 id="model-theory">Model Theory</h2>

<p>Most of the design-thinking in the ML process is in choosing a variety of models for comparing performance against.  The following three characteristics succinctly describe a ML model.</p>

<ol>
<li>Representation: structural model characteristics

<ul>
<li>name</li>
<li>family</li>
<li>interpretability</li>
<li>type

<ul>
<li>generative / discriminative</li>
<li>bias / var</li>
<li>fixed- / variable- learner</li>
</ul></li>
</ul></li>
<li>Evaluation: functions applied to the structure

<ul>
<li>objective</li>
<li>cost</li>
<li>loss</li>
</ul></li>
<li>Optimization: algorithms necessary to solve for parameters</li>
</ol>

<p>It is also important to understand how the chosen model effects the modeling process</p>

<ul>
<li>assumptions inherent in representation</li>
<li>alignment of loss function with project goals</li>
<li>sources of bias / variance</li>
<li>determination of resource constraints</li>
<li>enumeration of how over-tuning can occur (regularization)</li>
<li>understanding when manual methods are ineffective &lsquo;fiddling&rsquo; of model implementation parameters</li>
<li>statement of strong false assumptions can be better than weak true ones, because they need more data</li>
</ul>

<p><em>Note:</em> This should be considered carefully with feature engineering and feature selection to ensure the input transformations align with the model.</p>

<h2 id="the-machine-learning-process">The Machine Learning Process</h2>

<p>The following are the general steps taken in the ML Process.  They are similar to many other problem-solving and design-thinking processes, but tailored to ML specifics.</p>

<p>There a several hard checks that should be used to ensure the practicioner is maintaining honesty.  One important check is laying-out proper evaluation methods, before implementing them.  This is similar to classical statistics in choosing an accepatable p-value before running the model.</p>

<p>Another check is on model resource and time requirements.  More sophisticated models need more memory to implement and take a longer time to run.  These are highly dependent on the environment they are deployed to.  These should be determined with the customer, at the beginning.</p>

<p><em>Discover</em></p>

<ul>
<li>determine problem and constraints</li>
<li>determine characteristics the problem / scenario dictates on the solution

<ul>
<li>model family</li>
<li>acceptable methods of dimensionality reduction and regularization</li>
<li>deployment environment</li>
</ul></li>
<li>decide evaluation

<ul>
<li>primary / secondary evaluation score (ie. accuracy)</li>
<li>methods of evaluation (ie. confusion matrix, roc)</li>
</ul></li>
</ul>

<p><em>Collect and Transform</em></p>

<ul>
<li>obtain raw data

<ul>
<li>internal data warehouses</li>
<li>external APIs and services</li>
</ul></li>
<li>integration and cleaning<br /></li>
<li>filter, aggregate, and query</li>
</ul>

<p><em>Summary and Process</em></p>

<ul>
<li>exploration</li>
<li>preparation

<ul>
<li>address balance (classification, anova, etc.)</li>
<li>create Train, Validate, Test with split (above)</li>
</ul></li>
<li>configure Feature Extraction with feature_union</li>
<li>configure Preprocess and choose model-families with pipeline</li>
</ul>

<p><em>Build</em></p>

<ul>
<li>train the models

<ul>
<li>apply k-folds CV and grid search with Training set</li>
<li>perform on multiple model-families and hyper-parameters</li>
</ul></li>
<li>evaluate models

<ul>
<li>review afore-mentioned confusion matrix, scoring, classifier-threshold, and tests</li>
<li>select the best model-family / hyper-parameters</li>
<li>apply to Validation set or all of Training set to model-family to parameterize it and set as the final model</li>
</ul></li>
<li>refine performance

<ul>
<li>debug performance with learning curve, lift chart</li>
<li>use Testing set to evaluate final model characteristics</li>
<li>export to binary file</li>
</ul></li>
</ul>

<p><em>Deliver</em></p>

<ul>
<li>select solution

<ul>
<li>design interface most appropriate for using the model</li>
<li>automate data integration and pipelines</li>
<li>implement model in deployable environment</li>
</ul></li>
<li>deploy solution within system environments</li>
</ul>

<p><em>Note:</em> Train, Validate, and Test should be from different (independent) data sets, if possible.</p>

<h2 id="stakeholder-interaction-and-timeline">Stakeholder Interaction and Timeline</h2>

<p>It is useful to display these in relation to interactions that must take place with stakeholders.  These may be business users who need a problem solved, or technology departments that will have to support applications that implement the solution.  The y-axis show stage proximity to these stakeholders.</p>

<p>While every project is different, most stages use a similar proportion of time.  The horizontal axis lays-out the timeline.</p>

<p><img src="ml_process.png" alt="machine learning process" /></p>

<h2 id="demonstation">Demonstation</h2>

<p>The following code demonstrates the programming portions of these stages and steps using a toy example on a simulated diverse dataset of both numeric and categorical data.  It separates the columns and uses transform pipelines to perform different dimensionality reduction techniques.<sup class="footnote-ref" id="fnref:1"><a href="#fn:1">1</a></sup> This does display the important steps that must be taken with stakeholders.</p>

<h3 id="configuration">Configuration</h3>

<pre><code class="language-python">%matplotlib inline
</code></pre>

<pre><code class="language-python">import warnings
warnings.filterwarnings('ignore')

pd.set_option('display.max_rows', 10)
</code></pre>

<pre><code class="language-python">import random
import string

def randomString(stringLength=10):
    &quot;&quot;&quot;Generate a random string of fixed length &quot;&quot;&quot;
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(stringLength))

</code></pre>

<pre><code class="language-python">#modules
import pandas as pd
import numpy as np

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion

from sklearn.feature_selection import SelectKBest
from sklearn.decomposition import PCA

from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve
</code></pre>

<h3 id="collect-and-transform">Collect and Transform</h3>

<pre><code class="language-python">#make data (both numeric and categorical)
iCOLUMNS = 50
iROWS = 1000
#generate classification dataset
from sklearn.datasets.samples_generator import make_blobs
X_cat, Y_cat = make_blobs(n_samples=iROWS, centers=2, n_features=iCOLUMNS, random_state=1)
categories = [randomString(5) for x in range(5)]
cols = [random.choices(categories, k=iCOLUMNS) for _ in range(iROWS)]
X_cat = np.array(cols)


#generate regression dataset
from sklearn.datasets.samples_generator import make_regression
X_num, not_used = make_regression(n_samples=iROWS, n_features=iCOLUMNS, noise=0.1, random_state=1)
</code></pre>

<pre><code class="language-python">#set in dataframe
dfX_cat = pd.DataFrame(X_cat, columns=['IndepCat-'+str(x) for x in range(iCOLUMNS)] )
dfX_num = pd.DataFrame(X_num, columns=['IndepNum-'+str(x) for x in range(iCOLUMNS)])
dfY_cat = pd.DataFrame(Y_cat, columns=[&quot;Dep&quot;])
dfData = pd.concat([dfY_cat, dfX_cat, dfX_num], axis=1)
</code></pre>

<h3 id="summary-and-process">Summary and Process</h3>

<pre><code class="language-python">#separate
X = dfData.drop('Dep', axis=1)
y = dfData['Dep']

#encode y and create datasets
enc = LabelEncoder()
y_set = enc.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y_set, test_size=0.2)
</code></pre>

<pre><code class="language-python">print( &quot;Training records: %s&quot;%(X_train.shape[0]) )
print( &quot;Testing records: %s&quot;%(X_test.shape[0]) )
</code></pre>

<pre><code>Training records: 800
Testing records: 200
</code></pre>

<pre><code class="language-python">#create feature union of numeric data
features = []
features.append(('pca', PCA()))                    #&lt;&lt;&lt;GRID
features.append(('select_best', SelectKBest()))    #&lt;&lt;&lt;GRID
num_feature_eng = FeatureUnion(features)

#create the preprocessing pipelines for both numeric and categorical data
numeric_features = [x for x in X_train.columns if x.split('-')[0]=='IndepNum' ]
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('num_feature_eng', num_feature_eng)
])

categorical_features =  [x for x in X_train.columns if x.split('-')[0]=='IndepCat' ]
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore')),
    ('select_best', SelectKBest(k=6))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

#append classifier to preprocessing pipeline
#full prediction pipeline
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', LogisticRegression(solver='lbfgs'))])
</code></pre>

<pre><code class="language-python">param_grid = {
    'preprocessor__num__imputer__strategy': ['mean', 'median'],
    'classifier__C': [0.1, 1.0, 10, 100],
    'preprocessor__num__num_feature_eng__pca__n_components': [.75, .80, .85, .90, .95],
    'preprocessor__num__num_feature_eng__select_best__k': [5, 7, 9, 11]
}
gridClf = GridSearchCV(clf, param_grid, cv=5)
</code></pre>

<h3 id="build">Build</h3>

<pre><code class="language-python">gridClf.fit(X_train, y_train)
</code></pre>

<pre><code>GridSearchCV(cv=5, error_score='raise-deprecating',
       estimator=Pipeline(memory=None,
     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
         transformer_weights=None,
         transformers=[('num', Pipeline(memory=None,
     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,
       strategy='median', verbo...enalty='l2', random_state=None, solver='lbfgs',
          tol=0.0001, verbose=0, warm_start=False))]),
       fit_params=None, iid='warn', n_jobs=None,
       param_grid={'preprocessor__num__imputer__strategy': ['mean', 'median'], 'classifier__C': [0.1, 1.0, 10, 100], 'preprocessor__num__num_feature_eng__pca__n_components': [0.75, 0.8, 0.85, 0.9, 0.95], 'preprocessor__num__num_feature_eng__select_best__k': [5, 7, 9, 11]},
       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
       scoring=None, verbose=0)
</code></pre>

<pre><code class="language-python">print((&quot;best logistic regression from grid search: %.3f&quot;
       % gridClf.score(X_test, y_test)))
</code></pre>

<pre><code>best logistic regression from grid search: 0.515
</code></pre>

<pre><code class="language-python"># Cross Validation
#GridSearchCV: Mean cross-validated score of the best_estimator
#SGDClassifier: Returns the mean accuracy on the given test data and labels
print(&quot;Best Score: (CV score=%0.3f)&quot; % gridClf.best_score_)
print(&quot;\n&quot;)
print(&quot;Best Parameters\n %s&quot; % gridClf.best_params_ )
print(&quot;\n&quot;)
print(&quot;Best Estimator\n %s&quot; %  gridClf.best_estimator_ )
</code></pre>

<pre><code>Best Score: (CV score=0.484)


Best Parameters
 {'classifier__C': 10, 'preprocessor__num__imputer__strategy': 'mean', 'preprocessor__num__num_feature_eng__pca__n_components': 0.9, 'preprocessor__num__num_feature_eng__select_best__k': 5}


Best Estimator
 Pipeline(memory=None,
     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
         transformer_weights=None,
         transformers=[('num', Pipeline(memory=None,
     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',
       verbose...enalty='l2', random_state=None, solver='lbfgs',
          tol=0.0001, verbose=0, warm_start=False))])
</code></pre>

<pre><code class="language-python"># Confusion Matrix
from sklearn.metrics import confusion_matrix
y_train_pred = gridClf.predict(X_train)
print( &quot;Confusion matrix: Training&quot;)
confusion_matrix(y_train, y_train_pred)
</code></pre>

<pre><code>Confusion matrix: Training





array([[255, 149],
       [146, 250]])
</code></pre>

<pre><code class="language-python">y_pred = gridClf.predict(X_test)
print( &quot;Confusion matrix: Testing&quot;)
confusion_matrix(y_test, y_pred)
</code></pre>

<pre><code>Confusion matrix: Testing





array([[51, 45],
       [52, 52]])
</code></pre>

<pre><code class="language-python"># Testing

#predictions for outcome labels
#Predict class probabilities for X.  The predicted class probabilities of an input sample are computed as the mean predicted
#class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same
#class in a leaf.
y_class_prob = gridClf.predict_proba(X_test)                # called   predict_proba(), for some classifiers
y_prob = np.asarray( [x[1] for x in y_class_prob], dtype=np.float32)
threshold = 0                                               # set threshold
y_some_digit_pred = (y_prob &gt; threshold)
print( &quot;Average training probability: %0.3f&quot; % np.mean(y_prob) )

#roc auc
from sklearn.metrics import roc_auc_score
print( &quot;Area Under ROC Curve: %0.3f&quot; % roc_auc_score(y_test, y_prob) )
</code></pre>

<pre><code>Average training probability: 0.484
Area Under ROC Curve: 0.492
</code></pre>

<pre><code class="language-python">log_y_prob = gridClf.decision_function(X_test) 

from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt

#plt.figure(0).clf()

#LogisticRegression  
y_prob = log_y_prob
fpr, tpr, thresh = metrics.roc_curve(y_test, y_prob)
auc = metrics.roc_auc_score(y_test, y_prob)
plt.plot(fpr,tpr,label=&quot;logistic reg, auc=&quot;+str(auc))


plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc=0)
plt.show()
</code></pre>

<p><img src="output_27_0.png" alt="png" /></p>

<h3 id="deliver">Deliver</h3>

<pre><code class="language-python">from sklearn.externals import joblib
import os

cwd = os.getcwd()
file_path = cwd+'/Data/project/Models/'

#save the model to disk
filename = file_path+'finalized_model.sav'
joblib.dump(gridClf, filename)
</code></pre>

<pre><code class="language-python">#in deployment application...
#load the model from disk
loaded_model = joblib.load(filename)
result = loaded_model.score(X_test, Y_test)
print(result)
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>In this work, we provide an orthodox approach to the ML Process.  We list the stages involved and the steps for each, and align these with more traditional modeling and analysis processes.  Demo code runs through the basic ideas.  This code can be modified in order to automate processing in a separate environment, such as a deployed service.  The complete process can be generalized to many situations.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1"><a href="https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py">Column Transformer with Mixed Types</a>
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
</ol>
</div>

        </div>

        
        
        <div class="article-toc" style="display:none;">
            <h3>Contents</h3>
            <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#model-theory">Model Theory</a></li>
<li><a href="#the-machine-learning-process">The Machine Learning Process</a></li>
<li><a href="#stakeholder-interaction-and-timeline">Stakeholder Interaction and Timeline</a></li>
<li><a href="#demonstation">Demonstation</a>
<ul>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#collect-and-transform">Collect and Transform</a></li>
<li><a href="#summary-and-process">Summary and Process</a></li>
<li><a href="#build">Build</a></li>
<li><a href="#deliver">Deliver</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul>
</nav>
        </div>
        
        

        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.slim.min.js" integrity="sha256-/SIrNqv8h6QGKDuNoLGA4iret+kyesCkHGzVUUV0shc=" crossorigin="anonymous"></script>
        <script>
            (function() {
                var $toc = $('#TableOfContents');
                if ($toc.length > 0) {
                    var $window = $(window);

                    function onScroll(){
                        var currentScroll = $window.scrollTop();
                        var h = $('.article-entry h1, .article-entry h2, .article-entry h3, .article-entry h4, .article-entry h5, .article-entry h6');
                        var id = "";
                        h.each(function (i, e) {
                            e = $(e);
                            if (e.offset().top - 10 <= currentScroll) {
                                id = e.attr('id');
                            }
                        });
                        var active = $toc.find('a.active');
                        if (active.length == 1 && active.eq(0).attr('href') == '#' + id) return true;

                        active.each(function (i, e) {
                            $(e).removeClass('active').siblings('ul').hide();
                        });
                        $toc.find('a[href="#' + id + '"]').parentsUntil('#TableOfContents').each(function (i, e) {
                            $(e).children('a').addClass('active').siblings('ul').show();
                        });
                    }

                    $window.on('scroll', onScroll);
                    $(document).ready(function() {
                        $toc.find('a').parent('li').find('ul').hide();
                        onScroll();
                        document.getElementsByClassName('article-toc')[0].style.display = '';
                    });
                }
            })();
        </script>
        


        
        <footer class="article-footer">
            <ul class="article-tag-list">
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="https://imtorgdemo.github.io//tags/machine-learning">machine-learning
                    </a>
                </li>
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="https://imtorgdemo.github.io//tags/best-practice">best-practice
                    </a>
                </li>
                
            </ul>
        </footer>
        
    </div>
    <nav id="article-nav">
    
    
    <a href="/posts/post_test-hugo_blog/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Formatting for Jupyter (.ipynb) Notebooks&nbsp;<span>&gt;</span></div>
    </a>
    
</nav>

</article>

        
    </section>
    <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2019 IMTorg Blog Site
            <br />
            Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> with theme <a href="https://github.com/carsonip/hugo-theme-minos" target="_blank">Minos</a>
        </div>
    </div>
    

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/tomorrow-night.min.css" integrity="sha256-2wL88NKUqvJi/ExflDzkzUumjUM73mcK2gBvBBeLvTk=" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js" integrity="sha256-KbfTjB0WZ8vvXngdpJGY3Yp3xKk+tttbqClO11anCIU=" crossorigin="anonymous"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script>
        <script>renderMathInElement(document.body);</script>
    
    <script>
        document.getElementById('main-nav-toggle').addEventListener('click', function () {
            var header = document.getElementById('header');
            if (header.classList.contains('mobile-on')) {
                header.classList.remove('mobile-on');
            } else {
                header.classList.add('mobile-on');
            }
        });
    </script>
</footer>

</div>
</body>
</html>
