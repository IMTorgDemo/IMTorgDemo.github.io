<!DOCTYPE html>
<html>
<head>
    <title>Generalizing the Machine Learning Process // IMTorg Kbase</title>

        <meta charset="utf-8">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="author" content="">
        <meta property="og:title" content="Generalizing the Machine Learning Process" />
    <meta property="og:description" content="" />
    <meta property="og:type" content="website" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:url" content="https://imtorgdemo.github.io/posts/blog_models-machine_learning/" />
    

    <link href="" rel="alternate" type="application/rss+xml" title="IMTorg Kbase" />
    
    
    
    <link rel="apple-touch-icon" sizes="57x57" href="/favico/apple-icon-57x57.png">
	<link rel="apple-touch-icon" sizes="60x60" href="/favico/apple-icon-60x60.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/favico/apple-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="76x76" href="/favico/apple-icon-76x76.png">
	<link rel="apple-touch-icon" sizes="114x114" href="/favico/apple-icon-114x114.png">
	<link rel="apple-touch-icon" sizes="120x120" href="/favico/apple-icon-120x120.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/favico/apple-icon-144x144.png">
	<link rel="apple-touch-icon" sizes="152x152" href="/favico/apple-icon-152x152.png">
	<link rel="apple-touch-icon" sizes="180x180" href="/favico/apple-icon-180x180.png">
	<link rel="icon" type="image/png" sizes="192x192"  href="/android-icon-192x192.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favico/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favico/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favico/favicon-16x16.png">
	<link rel="manifest" href="/images/favico/manifest.json">
	<meta name="msapplication-TileColor" content="#ffffff">
	<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
	<meta name="theme-color" content="#ffffff">
    

    <link href="https://imtorgdemo.github.io/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
    <link href="https://imtorgdemo.github.io/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="https://imtorgdemo.github.io/css/style.css">

    <meta name="generator" content="Hugo 0.102.3" />
</head>


<body>
<div id="container">
    <header id="header">
    <div id="header-outer" class="outer">
        <div id="header-inner" class="inner">
            <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
            
            <a href="https://imtorgdemo.github.io/">
            	<img src="/logo_CA_newblue.png" alt="Logo" style="max-width:100px; padding-top: 10px">
            </a>
            <nav id="main-nav">
                
                <a class="main-nav-link" href="/pages/about/">About</a>
                
                <a class="main-nav-link" href="/categories/">Categories</a>
                
                <a class="main-nav-link" href="/pages/search/">Search</a>
                
            </nav>
            <nav id="sub-nav">
                <div id="search-form-wrap">
                </div>
            </nav>
        </div>
    </div>
</header>

    <section id="main" class="outer">
        <article class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        <header class="article-header">
            <h1 class="article-title" itemprop="name">Generalizing the Machine Learning Process</h1>
        </header>
        
        <div class="article-meta">
            <a href="/posts/blog_models-machine_learning/" class="article-date">
                <time datetime='2019-05-09T00:00:00.000&#43;00:00' itemprop="datePublished">2019-05-09</time>
            </a>
            
            
            <div class="post-categories">
                <div class="article-category">
                    
                    
                    <a class="article-category-link" href="https://imtorgdemo.github.io//categories/process">Process</a>
                    
                    
                    <span>&gt;</span>
                    
                    <a class="article-category-link" href="https://imtorgdemo.github.io//categories/datascience">DataScience</a>
                    
                </div>
            </div>
            
            
        </div>
        <div class="article-entry" itemprop="articleBody">
            <p>This work describes a general approach to follow when performing machine learning (ML) manually, and when automating in a deployment setting.  Unlike a classical statistical analysis, standard machine learning projects typically follow a general and repeatable process.  While the practictioner should be aware of details for each of the steps and the reasons for choosing them, there is much less design-thinking and checking of assumptions that are necessary components of more mathematical modeling fields.  This makes the machine learning process amenable to deployment as a service because automating the re-training and prediction of a model with consistent data is straight-forward programming.</p>
<h2 id="model-theory">Model Theory</h2>
<p>Most of the design-thinking in the ML process is in choosing a variety of models for comparing performance against.  The following three characteristics succinctly describe a ML model.  See <a href="https://imtorgdemo.github.io/posts/blog_page-todo/">here</a> for more explanation.</p>
<ol>
<li>Representation: structural model characteristics
<ul>
<li>name</li>
<li>family</li>
<li>interpretability</li>
<li>type
<ul>
<li>generative / discriminative</li>
<li>bias / var</li>
<li>fixed- / variable- learner</li>
</ul>
</li>
</ul>
</li>
<li>Evaluation: functions applied to the structure
<ul>
<li>objective</li>
<li>cost</li>
<li>loss</li>
</ul>
</li>
<li>Optimization: algorithms necessary to solve for parameters</li>
</ol>
<p>It is also important to understand how the chosen model effects the modeling process</p>
<ul>
<li>assumptions inherent in representation</li>
<li>alignment of loss function with project goals</li>
<li>sources of bias / variance</li>
<li>determination of resource constraints</li>
<li>enumeration of how over-tuning can occur (regularization)</li>
<li>understanding when manual methods are ineffective &lsquo;fiddling&rsquo; of model implementation parameters</li>
<li>statement of strong false assumptions can be better than weak true ones, because they need more data</li>
</ul>
<p><em>Note:</em> This should be considered carefully with feature engineering and feature selection to ensure the input transformations align with the model.</p>
<h2 id="the-machine-learning-process">The Machine Learning Process</h2>
<p>The following are the general steps taken in the ML Process.  They are similar to many other problem-solving processes, but tailored to ML specifics.  There are a few steps where care should be taken to ensure the process is reasonable.  These are more design-oriented and some research may be necessary depending on the scenario.</p>
<p>The practicioner must ensure he is maintaining honesty with the investigation.  One important check is laying-out proper evaluation methods, before implementing them.  This is similar to classical statistics in choosing an accepatable p-value before running the model.  They include selecting evaluation scores and how they should be evaluated.  Another check is on model resource and time requirements.  More sophisticated models need more memory to implement and take a longer time to run.  These are highly dependent on the environment they are deployed to.  These should be determined with the customer, at the beginning.  To learn more read, <a href="https://imtorgdemo.github.io/posts/blog_page-todo/">here</a> for more.</p>
<p>The categories to be predicted on, and the percent of records used for each in training, is a very important subject.  If your target data is of a much lower percentage amount than the rest (needle-in-haystack / imbalanced data problem), then it should be addressed before separating training and testing data.  The most orthodox and direct way to deal with this problem is to simply resample from the target records, with replacement, for enough times to reach 50% of your data.  However, this is an important area of research and some time should be taken before moving forward in the process.  For more information on imbalanced data, check <a href="https://imtorgdemo.github.io/posts/blog_page-todo/">here</a>.</p>
<p>While classical statistics places a strong emphasis on model exploration and ensuring assumptions are met, this is less important in ML.  Instead, much more time should be focused on the features used in the model.  This is mainly in the form of Feature Engineering, which can include several different methodologies, such as Feature Extraction, Feature Selection / Dimensionality Reduction, and Feature Engineering.  Read more about this, <a href="https://imtorgdemo.github.io/posts/blog_page-todo/">here</a>.</p>
<p><em>Discover</em></p>
<ul>
<li>determine problem and constraints</li>
<li>determine characteristics the problem / scenario dictates on the solution
<ul>
<li>model family</li>
<li>acceptable methods of dimensionality reduction and regularization</li>
<li>deployment environment</li>
</ul>
</li>
<li>decide evaluation
<ul>
<li>primary / secondary evaluation score (ie. accuracy)</li>
<li>methods of evaluation (ie. confusion matrix, roc)</li>
</ul>
</li>
</ul>
<p><em>Collect and Transform</em></p>
<ul>
<li>obtain raw data
<ul>
<li>internal data warehouses</li>
<li>external APIs and services</li>
</ul>
</li>
<li>integration and cleaning</li>
<li>filter, aggregate, and query</li>
</ul>
<p><em>Summary and Process</em></p>
<ul>
<li>exploration</li>
<li>preparation
<ul>
<li>address balance (classification, anova, etc.)</li>
<li>create Train, Validate, Test with split (above)</li>
</ul>
</li>
<li>configure Feature Extraction with feature_union</li>
<li>configure Preprocess and choose model-families with pipeline</li>
</ul>
<p><em>Build</em></p>
<ul>
<li>train the models
<ul>
<li>apply k-folds CV and grid search with Training set</li>
<li>perform on multiple model-families and hyper-parameters</li>
</ul>
</li>
<li>evaluate models
<ul>
<li>review afore-mentioned confusion matrix, scoring, classifier-threshold, and tests</li>
<li>select the best model-family / hyper-parameters</li>
<li>apply to Validation set or all of Training set to model-family to parameterize it and set as the final model</li>
</ul>
</li>
<li>refine performance
<ul>
<li>debug performance with learning curve, lift chart</li>
<li>use Testing set to evaluate final model characteristics</li>
<li>export to binary file</li>
</ul>
</li>
</ul>
<p><em>Deliver</em></p>
<ul>
<li>select solution
<ul>
<li>design interface most appropriate for using the model</li>
<li>automate data integration and pipelines</li>
<li>implement model in deployable environment</li>
</ul>
</li>
<li>deploy solution within system environments</li>
</ul>
<p><em>Note:</em> Train, Validate, and Test should be from different (independent) data sets, if possible.</p>
<h2 id="stakeholder-interaction-and-timeline">Stakeholder Interaction and Timeline</h2>
<p>It is useful to display these in relation to interactions that must take place with stakeholders.  These may be business users who need a problem solved, or technology departments that will have to support applications that implement the solution.  The y-axis show stage proximity to these stakeholders.</p>
<p>While every project is different, most stages use a similar proportion of time.  The horizontal axis lays-out the timeline.</p>
<p><img src="ml_process.png" alt="machine learning process"></p>
<h2 id="demonstation">Demonstation</h2>
<p>The following code demonstrates the programming portions of these stages and steps using a toy example on a simulated diverse dataset of both numeric and categorical data.  It separates the columns and uses transform pipelines to perform different dimensionality reduction techniques.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> This does display the important steps that must be taken with stakeholders.</p>
<h3 id="configuration">Configuration</h3>
<p>Notebook configurations for the kernel and display output.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> warnings
</span></span><span style="display:flex;"><span>warnings<span style="color:#f92672">.</span>filterwarnings(<span style="color:#e6db74">&#39;ignore&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pd<span style="color:#f92672">.</span>set_option(<span style="color:#e6db74">&#39;display.max_rows&#39;</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline
</span></span></code></pre></div><p>Provide for utility functions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> string
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">randomString</span>(stringLength<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Generate a random string of fixed length &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    letters <span style="color:#f92672">=</span> string<span style="color:#f92672">.</span>ascii_lowercase
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(random<span style="color:#f92672">.</span>choice(letters) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(stringLength))
</span></span></code></pre></div><p>While importing modules, try to organize them in the order they are used, and by their parent library.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#modules</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> LabelEncoder
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> OneHotEncoder
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.impute <span style="color:#f92672">import</span> SimpleImputer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.compose <span style="color:#f92672">import</span> ColumnTransformer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.pipeline <span style="color:#f92672">import</span> Pipeline
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.pipeline <span style="color:#f92672">import</span> FeatureUnion
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.feature_selection <span style="color:#f92672">import</span> SelectKBest
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> KFold
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> cross_val_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> GridSearchCV
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> learning_curve
</span></span></code></pre></div><h3 id="collect-and-transform">Collect and transform</h3>
<p>Bring internal and external data, together, for a combined and succinct dataset.  Here, we are simulating both categorical and numeric data for the same dataset.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#make data (both numeric and categorical)</span>
</span></span><span style="display:flex;"><span>iCOLUMNS <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>iROWS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#generate classification dataset</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets.samples_generator <span style="color:#f92672">import</span> make_blobs
</span></span><span style="display:flex;"><span>X_cat, Y_cat <span style="color:#f92672">=</span> make_blobs(n_samples<span style="color:#f92672">=</span>iROWS, centers<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, n_features<span style="color:#f92672">=</span>iCOLUMNS, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>categories <span style="color:#f92672">=</span> [randomString(<span style="color:#ae81ff">5</span>) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">5</span>)]
</span></span><span style="display:flex;"><span>cols <span style="color:#f92672">=</span> [random<span style="color:#f92672">.</span>choices(categories, k<span style="color:#f92672">=</span>iCOLUMNS) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(iROWS)]
</span></span><span style="display:flex;"><span>X_cat <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(cols)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#generate regression dataset</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets.samples_generator <span style="color:#f92672">import</span> make_regression
</span></span><span style="display:flex;"><span>X_num, not_used <span style="color:#f92672">=</span> make_regression(n_samples<span style="color:#f92672">=</span>iROWS, n_features<span style="color:#f92672">=</span>iCOLUMNS, noise<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Your processing is simplified when most of your work is within a dataframe.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#set in dataframe</span>
</span></span><span style="display:flex;"><span>dfX_cat <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_cat, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;IndepCat-&#39;</span><span style="color:#f92672">+</span>str(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(iCOLUMNS)] )
</span></span><span style="display:flex;"><span>dfX_num <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_num, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;IndepNum-&#39;</span><span style="color:#f92672">+</span>str(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(iCOLUMNS)])
</span></span><span style="display:flex;"><span>dfY_cat <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(Y_cat, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Dep&#34;</span>])
</span></span><span style="display:flex;"><span>dfData <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([dfY_cat, dfX_cat, dfX_num], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><h3 id="summary-and-process">Summary and process</h3>
<p>Encode your data with transformations in an appropriate order.  Remember that some transformations, such as scaling, should be performed after separation of testing and training datasets because you will be dividing all records by the maximum value, within the dataset.  In model implementation, you would not see the maximum value of your original testing dataset; rather, you would use the current dataset.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#separate</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> dfData<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#39;Dep&#39;</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> dfData[<span style="color:#e6db74">&#39;Dep&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#encode y and create datasets</span>
</span></span><span style="display:flex;"><span>enc <span style="color:#f92672">=</span> LabelEncoder()
</span></span><span style="display:flex;"><span>y_set <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>fit_transform(y)
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y_set, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print( <span style="color:#e6db74">&#34;Training records: </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">%</span>(X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]) )
</span></span><span style="display:flex;"><span>print( <span style="color:#e6db74">&#34;Testing records: </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">%</span>(X_test<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]) )
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Training records: 800
Testing records: 200
</code></pre>

</div>

<p>Here, you perform scaling, imputation, Principal Component Analysis (PCA), and other transformations whose arguments are dependent upon the same dataset.</p>
<p>Also, the transformation pipelines should be divided into numeric and categoric because some transformations, such as PCA, are inappropriate for categoric data.  Instead use methods, such as <em>Select K Best</em>, for dimensionality reduction.</p>
<p>Working with pipelines can be unwieldly at some times.  For a reference of pipelines, see <a href="https://scikit-learn.org/stable/modules/compose.html#pipelines-and-composite-estimators">sklearn docs</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#create feature union of numeric data</span>
</span></span><span style="display:flex;"><span>features <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>features<span style="color:#f92672">.</span>append((<span style="color:#e6db74">&#39;pca&#39;</span>, PCA()))                    <span style="color:#75715e">#&lt;&lt;&lt;GRID</span>
</span></span><span style="display:flex;"><span>features<span style="color:#f92672">.</span>append((<span style="color:#e6db74">&#39;select_best&#39;</span>, SelectKBest()))    <span style="color:#75715e">#&lt;&lt;&lt;GRID</span>
</span></span><span style="display:flex;"><span>num_feature_eng <span style="color:#f92672">=</span> FeatureUnion(features)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#create the preprocessing pipelines for both numeric and categorical data</span>
</span></span><span style="display:flex;"><span>numeric_features <span style="color:#f92672">=</span> [x <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> X_train<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> x<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;-&#39;</span>)[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;IndepNum&#39;</span> ]
</span></span><span style="display:flex;"><span>numeric_transformer <span style="color:#f92672">=</span> Pipeline(steps<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;imputer&#39;</span>, SimpleImputer(strategy<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;median&#39;</span>)),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;scaler&#39;</span>, StandardScaler()),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;num_feature_eng&#39;</span>, num_feature_eng)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>categorical_features <span style="color:#f92672">=</span>  [x <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> X_train<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> x<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;-&#39;</span>)[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;IndepCat&#39;</span> ]
</span></span><span style="display:flex;"><span>categorical_transformer <span style="color:#f92672">=</span> Pipeline(steps<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;imputer&#39;</span>, SimpleImputer(strategy<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;constant&#39;</span>, fill_value<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;missing&#39;</span>)),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;onehot&#39;</span>, OneHotEncoder(handle_unknown<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ignore&#39;</span>)),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#39;select_best&#39;</span>, SelectKBest(k<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>preprocessor <span style="color:#f92672">=</span> ColumnTransformer(
</span></span><span style="display:flex;"><span>    transformers<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#39;num&#39;</span>, numeric_transformer, numeric_features),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#39;cat&#39;</span>, categorical_transformer, categorical_features)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#append classifier to preprocessing pipeline</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#full prediction pipeline</span>
</span></span><span style="display:flex;"><span>clf <span style="color:#f92672">=</span> Pipeline(steps<span style="color:#f92672">=</span>[(<span style="color:#e6db74">&#39;preprocessor&#39;</span>, preprocessor),
</span></span><span style="display:flex;"><span>                      (<span style="color:#e6db74">&#39;classifier&#39;</span>, LogisticRegression(solver<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lbfgs&#39;</span>))])
</span></span></code></pre></div><p>Optimize your hyper-parameters in a grid search.  You may have to get output for all the pipeline keys to find their names because the notation can become quite lenthy based on the amount of nesting, such as this pipeline displays.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>param_grid <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;preprocessor__num__imputer__strategy&#39;</span>: [<span style="color:#e6db74">&#39;mean&#39;</span>, <span style="color:#e6db74">&#39;median&#39;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;classifier__C&#39;</span>: [<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">100</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;preprocessor__num__num_feature_eng__pca__n_components&#39;</span>: [<span style="color:#ae81ff">.75</span>, <span style="color:#ae81ff">.80</span>, <span style="color:#ae81ff">.85</span>, <span style="color:#ae81ff">.90</span>, <span style="color:#ae81ff">.95</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;preprocessor__num__num_feature_eng__select_best__k&#39;</span>: [<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">11</span>]
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>gridClf <span style="color:#f92672">=</span> GridSearchCV(clf, param_grid, cv<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><h3 id="build">Build</h3>
<p>Build your model through optimizing values over the grid.  This is usually based on stochastic gradient descent, or some other iterative routine.  Convergence may be time-consuming depending on the model you&rsquo;ve chosen and the number of parameters and hyper-parameters that must be selected.  NaiveBayes methods can be some of the most costly.  Run more balanced models, first, to get an idea of the parameter space, before trying more costly approaches.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>gridClf<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;,
       estimator=Pipeline(memory=None,
     steps=[(&#39;preprocessor&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3,
         transformer_weights=None,
         transformers=[(&#39;num&#39;, Pipeline(memory=None,
     steps=[(&#39;imputer&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan,
       strategy=&#39;median&#39;, verbo...enalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;,
          tol=0.0001, verbose=0, warm_start=False))]),
       fit_params=None, iid=&#39;warn&#39;, n_jobs=None,
       param_grid={&#39;preprocessor__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;], &#39;classifier__C&#39;: [0.1, 1.0, 10, 100], &#39;preprocessor__num__num_feature_eng__pca__n_components&#39;: [0.75, 0.8, 0.85, 0.9, 0.95], &#39;preprocessor__num__num_feature_eng__select_best__k&#39;: [5, 7, 9, 11]},
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=None, verbose=0)
</code></pre>

</div>

<p>Take the <em>best</em> model with parameters / hyper-parameters from highest grid-search, cross-validation, and apply it to the test data.  This data is simulated and you should be able to perform better than a coin-flip (50%).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print((<span style="color:#e6db74">&#34;best logistic regression from grid search: </span><span style="color:#e6db74">%.3f</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">%</span> gridClf<span style="color:#f92672">.</span>score(X_test, y_test)))
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">best logistic regression from grid search: 0.515
</code></pre>

</div>

<p>Get an idea of the results and parameter space.  Extreme values may indicate over-fitting or some other anomaly is occuring.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Cross Validation</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#GridSearchCV: Mean cross-validated score of the best_estimator</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#SGDClassifier: Returns the mean accuracy on the given test data and labels</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Best Score: (CV score=</span><span style="color:#e6db74">%0.3f</span><span style="color:#e6db74">)&#34;</span> <span style="color:#f92672">%</span> gridClf<span style="color:#f92672">.</span>best_score_)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Best Parameters</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> gridClf<span style="color:#f92672">.</span>best_params_ )
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Best Estimator</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span>  gridClf<span style="color:#f92672">.</span>best_estimator_ )
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Best Score: (CV score=0.484)


Best Parameters
 {&#39;classifier__C&#39;: 10, &#39;preprocessor__num__imputer__strategy&#39;: &#39;mean&#39;, &#39;preprocessor__num__num_feature_eng__pca__n_components&#39;: 0.9, &#39;preprocessor__num__num_feature_eng__select_best__k&#39;: 5}


Best Estimator
 Pipeline(memory=None,
     steps=[(&#39;preprocessor&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3,
         transformer_weights=None,
         transformers=[(&#39;num&#39;, Pipeline(memory=None,
     steps=[(&#39;imputer&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy=&#39;mean&#39;,
       verbose...enalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;,
          tol=0.0001, verbose=0, warm_start=False))])
</code></pre>

</div>

<p>A confusion matrix is your basic display of results.  This is for your training data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Confusion Matrix</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> confusion_matrix
</span></span><span style="display:flex;"><span>y_train_pred <span style="color:#f92672">=</span> gridClf<span style="color:#f92672">.</span>predict(X_train)
</span></span><span style="display:flex;"><span>print( <span style="color:#e6db74">&#34;Confusion matrix: Training&#34;</span>)
</span></span><span style="display:flex;"><span>confusion_matrix(y_train, y_train_pred)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Confusion matrix: Training
</code></pre>

</div>

<div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">array([[255, 149],
       [146, 250]])
</code></pre>

</div>

<p>This confusion matrix is for your test data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> gridClf<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>print( <span style="color:#e6db74">&#34;Confusion matrix: Testing&#34;</span>)
</span></span><span style="display:flex;"><span>confusion_matrix(y_test, y_pred)
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Confusion matrix: Testing
</code></pre>

</div>

<div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">array([[51, 45],
       [52, 52]])
</code></pre>

</div>

<p>After getting a general feel for the situation, look at the specific performance metrics you set, at the beginning of your problem statement.  Ensure you hold yourself to this standard and do not change the solutions&rsquo; needs based on current results.  For a complete listing of performance metrics, view the <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation-quantifying-the-quality-of-predictions">sklearn docs</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Testing</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#predictions for outcome labels</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Predict class probabilities for X.  The predicted class probabilities of an input sample are computed as the mean predicted</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#class in a leaf.</span>
</span></span><span style="display:flex;"><span>y_class_prob <span style="color:#f92672">=</span> gridClf<span style="color:#f92672">.</span>predict_proba(X_test)                <span style="color:#75715e"># called   predict_proba(), for some classifiers</span>
</span></span><span style="display:flex;"><span>y_prob <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray( [x[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> y_class_prob], dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>threshold <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>                                               <span style="color:#75715e"># set threshold</span>
</span></span><span style="display:flex;"><span>y_some_digit_pred <span style="color:#f92672">=</span> (y_prob <span style="color:#f92672">&gt;</span> threshold)
</span></span><span style="display:flex;"><span>print( <span style="color:#e6db74">&#34;Average training probability: </span><span style="color:#e6db74">%0.3f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> np<span style="color:#f92672">.</span>mean(y_prob) )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#roc auc</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> roc_auc_score
</span></span><span style="display:flex;"><span>print( <span style="color:#e6db74">&#34;Area Under ROC Curve: </span><span style="color:#e6db74">%0.3f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> roc_auc_score(y_test, y_prob) )
</span></span></code></pre></div><div class="output">

<pre tabindex="0"><code class="language-nb-output" data-lang="nb-output">Average training probability: 0.484
Area Under ROC Curve: 0.492
</code></pre>

</div>

<p>A variety of different plots can be used to explore model and result performance.  The ROC Curve is one of the most fundamental, but several others are of importance.  Performance graphs are explained in the <a href="https://scikit-learn.org/stable/modules/learning_curve.html#validation-curves-plotting-scores-to-evaluate-models">sklearn docs</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>log_y_prob <span style="color:#f92672">=</span> gridClf<span style="color:#f92672">.</span>decision_function(X_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> metrics
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#plt.figure(0).clf()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#LogisticRegression  </span>
</span></span><span style="display:flex;"><span>y_prob <span style="color:#f92672">=</span> log_y_prob
</span></span><span style="display:flex;"><span>fpr, tpr, thresh <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>roc_curve(y_test, y_prob)
</span></span><span style="display:flex;"><span>auc <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>roc_auc_score(y_test, y_prob)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(fpr,tpr,label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;logistic reg, auc=&#34;</span><span style="color:#f92672">+</span>str(auc))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;False Positive Rate&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;True Positive Rate&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Receiver operating characteristic&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="output_40_0.png" alt="png"></p>
<h3 id="deliver">Deliver</h3>
<p>Save the model for future use, especially if you just spent a large amount of time getting the algorithm to converge.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.externals <span style="color:#f92672">import</span> joblib
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cwd <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getcwd()
</span></span><span style="display:flex;"><span>file_path <span style="color:#f92672">=</span> cwd<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;/Data/project/Models/&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#save the model to disk</span>
</span></span><span style="display:flex;"><span>filename <span style="color:#f92672">=</span> file_path<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;finalized_model.sav&#39;</span>
</span></span><span style="display:flex;"><span>joblib<span style="color:#f92672">.</span>dump(gridClf, filename)
</span></span></code></pre></div><p>Implement the model within the necessary solution system.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#in deployment application...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#load the model from disk</span>
</span></span><span style="display:flex;"><span>loaded_model <span style="color:#f92672">=</span> joblib<span style="color:#f92672">.</span>load(filename)
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> loaded_model<span style="color:#f92672">.</span>score(X_test, Y_test)
</span></span><span style="display:flex;"><span>print(result)
</span></span></code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>In this work, we provide an orthodox approach to the ML Process.  We list the stages involved and the steps for each, and align these with more traditional modeling and analysis processes.  Demo code runs through the basic ideas.  This code can be modified in order to automate processing in a separate environment, such as a deployed service.  The complete process can be generalized to many situations.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py">Column Transformer with Mixed Types</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

        </div>

        
        
        <div class="article-toc" style="display:none;">
            <h3>Contents</h3>
            <nav id="TableOfContents">
  <ul>
    <li><a href="#model-theory">Model Theory</a></li>
    <li><a href="#the-machine-learning-process">The Machine Learning Process</a></li>
    <li><a href="#stakeholder-interaction-and-timeline">Stakeholder Interaction and Timeline</a></li>
    <li><a href="#demonstation">Demonstation</a>
      <ul>
        <li><a href="#configuration">Configuration</a></li>
        <li><a href="#collect-and-transform">Collect and transform</a></li>
        <li><a href="#summary-and-process">Summary and process</a></li>
        <li><a href="#build">Build</a></li>
        <li><a href="#deliver">Deliver</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
        </div>
        
        

        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.slim.min.js" integrity="sha256-/SIrNqv8h6QGKDuNoLGA4iret+kyesCkHGzVUUV0shc=" crossorigin="anonymous"></script>
        <script>
            (function() {
                var $toc = $('#TableOfContents');
                if ($toc.length > 0) {
                    var $window = $(window);

                    function onScroll(){
                        var currentScroll = $window.scrollTop();
                        var h = $('.article-entry h1, .article-entry h2, .article-entry h3, .article-entry h4, .article-entry h5, .article-entry h6');
                        var id = "";
                        h.each(function (i, e) {
                            e = $(e);
                            if (e.offset().top - 10 <= currentScroll) {
                                id = e.attr('id');
                            }
                        });
                        var active = $toc.find('a.active');
                        if (active.length == 1 && active.eq(0).attr('href') == '#' + id) return true;

                        active.each(function (i, e) {
                            $(e).removeClass('active').siblings('ul').hide();
                        });
                        $toc.find('a[href="#' + id + '"]').parentsUntil('#TableOfContents').each(function (i, e) {
                            $(e).children('a').addClass('active').siblings('ul').show();
                        });
                    }

                    $window.on('scroll', onScroll);
                    $(document).ready(function() {
                        $toc.find('a').parent('li').find('ul').hide();
                        onScroll();
                        document.getElementsByClassName('article-toc')[0].style.display = '';
                    });
                }
            })();
        </script>
        


        
        <footer class="article-footer">
            <ul class="article-tag-list">
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="https://imtorgdemo.github.io//tags/machine-learning">machine-learning
                    </a>
                </li>
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="https://imtorgdemo.github.io//tags/best-practice">best-practice
                    </a>
                </li>
                
            </ul>
        </footer>
        
    </div>
    <nav id="article-nav">
    
    <a href="/posts/blog_nlp-regular_expressions_workflow/" id="article-nav-newer" class="article-nav-link-wrap">
        <div class="article-nav-title"><span>&lt;</span>&nbsp;
            Solving Textual Problems with Regular Expressions
        </div>
    </a>
    
    
    <a href="/posts/blog_test-hugo_blog/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Formatting for Jupyter (.ipynb) Notebooks&nbsp;<span>&gt;</span></div>
    </a>
    
</nav>

</article>

        
    </section>
    <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2023 IMTorg Kbase
            <br />
            <b> Want to discuss software?</b><br>Send me a message <a href="mailto:information@mgmt-tech.org?Subject=Open%20Software" target="_top"></a> information@mgmt-tech.org
        </div>
    </div>
    

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/tomorrow-night.min.css" integrity="sha256-2wL88NKUqvJi/ExflDzkzUumjUM73mcK2gBvBBeLvTk=" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js" integrity="sha256-KbfTjB0WZ8vvXngdpJGY3Yp3xKk+tttbqClO11anCIU=" crossorigin="anonymous"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script>
        <script>renderMathInElement(document.body);</script>
    
    <script>
        document.getElementById('main-nav-toggle').addEventListener('click', function () {
            var header = document.getElementById('header');
            if (header.classList.contains('mobile-on')) {
                header.classList.remove('mobile-on');
            } else {
                header.classList.add('mobile-on');
            }
        });
    </script>
    <script 
	src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.8/lunr.min.js" 
	integrity="sha256-34Si1Y6llMBKM3G0jQILVeoQKEwuxjbk4zGWXXMT4ps=" 
	crossorigin="anonymous"></script>
	<script src="https://imtorgdemo.github.io/js/search.js"></script>
</footer>
</div>
</body>
</html>
