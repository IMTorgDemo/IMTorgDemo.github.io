<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nlp on IMTorg Kbase</title>
    <link>https://imtorgdemo.github.io/tags/nlp/</link>
    <description>Recent content in nlp on IMTorg Kbase</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://imtorgdemo.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Determine a Reasonable Sample Size for Training a Text Classifier</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-sample_size_pred_intvl/</link>
      <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-sample_size_pred_intvl/</guid>
      <description>Consider that we are creating a text classification model that will discover a few target needles in a massive haystack of records. Maybe we are looking for messages of Death among emergency services messages - 20 for every 50,000 messages. Labeling this large dataset and finding the Death messages is costly, so we don&amp;rsquo;t want to label more than is necessary. If we already have enough to estimate the target proportion, let us determine whether the sample size used for training the classification model is large enough.</description>
    </item>
    
    <item>
      <title>Terminology Useful for NLP</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-intro_terminology/</link>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-intro_terminology/</guid>
      <description>NLP allows for both theoretical and practical study of language. Below are a few of aspects of study, as well as terminology, that is frequently used within the field. Because of the interdisciplinary nature of computational linguistics, the terms come from linguistics, computer science, and mathematics.
Semiotics - a philosophical theory covering the relationship between signs and the things they reference.
Phonetic and Phonological Knowledge - Phonetics is the study of language at the level of sounds while phonology is the study of the combination of sounds into organized units of speech.</description>
    </item>
    
    <item>
      <title>Determining Sample Size for AI Models</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-sample_size_calc/</link>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-sample_size_calc/</guid>
      <description>We are going to dive into the deep disturbing world of sample size in AI. This work is RARELY done as part of AI solutions. There are comparatively few research papers on this topic, and the approaches they offer tend to be specific to the underlying problem addressed in the paper. However, the simple fact that it is poorly understood gives great understanding to the world of AI, which is why I describe it as &amp;lsquo;disturbing&amp;rsquo;.</description>
    </item>
    
    <item>
      <title>SpaCy Internals</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-spacy_internals/</link>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-spacy_internals/</guid>
      <description>I think there is some confusion about the different components - I&amp;rsquo;ll try to clarify:
The tokenizer does not produce vectors. It&amp;rsquo;s just a component that segments texts into tokens. In spaCy, it&amp;rsquo;s rule-based and not trainable, and doesn&amp;rsquo;t have anything to do with vectors. It looks at whitespace and punctuation to determine which are the unique tokens in a sentence.
An nlp model in spaCy can have predefined (static) word vectors that are accessible on the Token level.</description>
    </item>
    
    <item>
      <title>Ranking Text With Word Embeddings</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-rank_text_with_word_vector/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-rank_text_with_word_vector/</guid>
      <description>I recently implemented search functionality for my Hugo site, which can be seen at: https://imtorgdemo.github.io/pages/search/. The search uses lunr.js, an implementation of Solr. While it works, sufficiently, the metadata used for ranking queries could be improved. It would also be nice to visually locate the results by where resulting posts fit into the three data science fundamental disciplines: mathematics, computer science, and business. This narrative provides a quick solution for ranking posts by each discipline, then reducing the dimensions to 3 axes in the xy-plane.</description>
    </item>
    
    <item>
      <title>Incorporating an Initial Training Sample into a Project</title>
      <link>https://imtorgdemo.github.io/posts/blog_math-reuse_tng_sample/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_math-reuse_tng_sample/</guid>
      <description>During a data science project, data is often provided in an incremental manner. Some customer files are easier to obtain than others, such as when lengthy unarchiving processes are warranted. To ensure no time is wasted, available data can be put to use with initial analyses and model training as a Training Sample. The same data is incorporated with Training data when it is formally split into Training and Holdout sets.</description>
    </item>
    
    <item>
      <title>Processing Natural Language with Python and Friends</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-intro_spacy/</link>
      <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-intro_spacy/</guid>
      <description>Python is a typical language chosen for Data Science work, and its strengths with strings make it especially useful for working with natural language. While the nltk library opened-up this work for python users, the newer spacy improves upon processing power by implementing Cython code. Tests display its power in production when compared with more traditional approaches, such as with Stanford&amp;rsquo;s CoreNLP. This post is an outline of examples from the spacy coursework and examples.</description>
    </item>
    
    <item>
      <title>Historical Background of NLP with Deep Learning</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-history_nlp_deep_learn/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-history_nlp_deep_learn/</guid>
      <description>The automated linguistic annotation of natural languages kept linguists and computer scientists hard at work for many decades. This work focused on the syntax of language as well as basic understanding and includes part-of-speech, named entity categorization, and syntatic dependency. Language meta-data tagging became much more accurate with the introduction of neural network and deep learning models. Because pairing meta-data with more powerful models is sure to allow for an explosion of new applications, it is important to understand the developments that allowed for the creation of this technology.</description>
    </item>
    
    <item>
      <title>Explaining Difficult (Abstract) Subjects to Customers</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-explain_metrics/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-explain_metrics/</guid>
      <description>Stakeholders want to understand what you are selling and how your approach is different. However, the level of detail they want varies greatly among your different customers. A simple whitepaper can be useful enough, in most situations. But, occaisionally, a layman will want to know exactly what you are doing and will not be content until you very succinctly explain it to them. This post will explain two methods for approaching such obstacles.</description>
    </item>
    
    <item>
      <title>Building Math from the Ground-Up</title>
      <link>https://imtorgdemo.github.io/posts/blog-logic_for_math/</link>
      <pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog-logic_for_math/</guid>
      <description>After completing basic high school mathematics, one might think that math stands on its own. To understand and know that 1+1=2 is obvious. But math can go much deeper. In fact, we can build new concepts from math as well as build a math, itself. This is important because it effects how we think about other subjects, and is especially useful in helping us think about language and abstract concepts, such as in programming.</description>
    </item>
    
    <item>
      <title>Solving Textual Problems with Regular Expressions</title>
      <link>https://imtorgdemo.github.io/posts/blog_nlp-regular_expressions_workflow/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://imtorgdemo.github.io/posts/blog_nlp-regular_expressions_workflow/</guid>
      <description>Regular Expressions provide an important foundation for learning systems. They are useful for quick and direct approaches to solving problems without creating mounds of training data, nor the infrastructure for deploying a model. While they are a common programming technique, and simple enough to employ, they tend to be used so infrequently that you must re-learn them each time you wish to apply. This post summarizes the basic regex syntax, strategies, and workflow in hopes it will decrease the time needed to implement.</description>
    </item>
    
  </channel>
</rss>
